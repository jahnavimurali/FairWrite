{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVTa2FYd7gY-",
        "outputId": "d9eec428-be8d-4799-9de2-04ce1bc6d673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlFOldHlrBQ0"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
      ],
      "metadata": {
        "id": "_gjJjR8BQTHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "qQC1xM1ycHua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Load the original dataset\n",
        "input_file_path = \"/content/drive/Shareddrives/FYP 2024-2025/Phase-2/News_MBIC/Processing/rephrase-dataset.csv\"   # Replace with your dataset file path\n",
        "df = pd.read_csv(input_file_path)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, rem = train_test_split(df, train_size=0.8, random_state=42)\n",
        "val, test = train_test_split(rem, train_size=0.5, random_state=42)\n"
      ],
      "metadata": {
        "id": "CXg1plwQ2fwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TinyLLaMa\n"
      ],
      "metadata": {
        "id": "vicDvFgOQdzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Loading\n"
      ],
      "metadata": {
        "id": "TSAuWD88UWLm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "d2afd85c-aa2e-4be9-cf98-bce87338c9ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.1.7: Fast Llama patching. Transformers: 4.47.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/tinyllama-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "7d4b2835-df6e-47e8-cd52-165407fda64c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Training embed_tokens in mixed precision to save VRAM\n",
            "Unsloth: Training lm_head in mixed precision to save VRAM\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",\"lm_head\",\"embed_tokens\",],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0, # Currently only supports dropout = 0\n",
        "    bias = \"none\",    # Currently only supports bias = \"none\"\n",
        "    use_gradient_checkpointing = False, # @@@ IF YOU GET OUT OF MEMORY - set to True @@@\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "JCnYZ6xyUS0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "train_dataset = Dataset.from_pandas(train)\n",
        "test_dataset = Dataset.from_pandas(test)\n",
        "val_dataset = Dataset.from_pandas(val)"
      ],
      "metadata": {
        "id": "I5q86WMjfDve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "4f0572eea5e14352aaff175489138cc8",
            "880a1ffe30d443df86cb3c23a92963d7",
            "372a5c75fcdb4b19bb0a190ffa71dbce",
            "1e0db999c48e410eb9378d4389dc870f",
            "0a523788b23241e0b47657aa079b588b",
            "489031710b0a4ad8b0ed43881b111762",
            "ce930d3161a1449783bc4de608c52caf",
            "7dcead23c6d0452a99ed4b08aa7232b5",
            "c2058fce56dc48b68d76794ef7e7adf8",
            "2ee3b51742d740718961b33605643949",
            "d9d8c62e89b14ea6a5464a14512780d5"
          ]
        },
        "id": "LjY75GoYUCB8",
        "outputId": "59dc4a86-27f2-4f40-efa1-283464b7b5ea"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f0572eea5e14352aaff175489138cc8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Rephrase the sentence to remove linguistic bias while retaining its original meaning and structure. Ensure the rephrased sentence is neutral and professional.\n",
            "\n",
            "### Input:\n",
            "Biased Sentence:But if you major in philosophy or gender studies, you might struggle to find any job in your field, let alone a well-paying one.| Biased Words:['struggle']\n",
            "\n",
            "### Response:\n",
            "Debiased Sentence:But if you major in philosophy or gender studies, you might find it challenging to secure a job in your field, let alone a well-paying one.</s>\n"
          ]
        }
      ],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = \"Rephrase the sentence to remove linguistic bias while retaining its original meaning and structure. Ensure the rephrased sentence is neutral and professional.\"\n",
        "    inputs = [\"Biased Sentence:\"+ biased_sentence + \"| Biased Words:\" + str(biased_words) for biased_sentence, biased_words in zip(examples[\"biased_sentence\"], examples[\"biased_words\"])]\n",
        "    outputs = [\"Debiased Sentence:\"+str(debiased_sentence) for debiased_sentence in examples[\"debiased_sentence\"]]\n",
        "    # outputs = [str(examples['debiased_sentence'])\n",
        "    texts = []\n",
        "    for input, output in zip(inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instructions, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    print(texts[0])\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "# dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
        "tok_dataset = train_dataset.map(formatting_prompts_func,batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "dEa78fgAUOgg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "# Define Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = tok_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = True, # Packs short sequences together to save time!\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_ratio = 0.1,\n",
        "        num_train_epochs = 19,\n",
        "        max_steps = 30,\n",
        "        learning_rate = 2e-5,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.1,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "aa82b60fddde42ff8640d34c5cffcc98",
            "c28a893daa6b461dbb2e8bb1f1359fb1",
            "1b15e1dc3de647818e82dec82bf37085",
            "ca413fe0a6d84385b3469aa575e4af99",
            "e49928a1074c4700ac344c472efc57c7",
            "e709f6129be54bb2a4786f1d5f6f3966",
            "6067f2526fab495b89f50d3a592938a1",
            "e13047a6f76044068b94a5e5eadeb9b6",
            "17cf9c82e5b34096b67af66b4022407f",
            "553acf71c7d8435589905917dcbd7c8c",
            "d4d108cc8f5d4a99a5ede6246ebbac64"
          ]
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "6c2d9e5a-afae-4be4-ae16-4f36e1eea03b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Counting untrained tokens:   0%|          | 0/20 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa82b60fddde42ff8640d34c5cffcc98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Setting embed_tokens & lm_head untrained tokens to mean(trained) to counteract NaNs during training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 20 | Num Epochs = 15\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 30\n",
            " \"-____-\"     Number of trainable parameters = 156,303,360\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [30/30 06:06, Epoch 10/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>5.480600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>5.499100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>5.518800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>5.496000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>5.485300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>5.515600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>5.455300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>5.355100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>5.396500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>5.266800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>5.127200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>5.084000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>5.052900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>4.980100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>4.911700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>4.813300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>4.814300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>4.768500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>4.704600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>4.641900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>4.582500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>4.560000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>4.537500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>4.498100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>4.484700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>4.435200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>4.441800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>4.460700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>4.389200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>4.345600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n"
      ],
      "metadata": {
        "id": "sJOWuqY0ULMj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "4bfeda0e-754c-4405-eeda-d764fbb83fd1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nRephrase the sentence to remove linguistic bias while retaining its original meaning and structure. Ensure the rephrased sentence is neutral and professional.|\\n\\n### Input:\\nBiased sentence: On Tuesday, President Donald Trump will give an Oval Office address on the government shutdown he created, during which he presumably will lie through his teeth to defend his highly unpopular immigration agenda.| Biased Words: ['unpopular', 'lie', 'through', 'his', 'teeth', 'highly']\\n\\n### Response:\\nRephrase the sentence to remove linguistic bias while retaining its original meaning and structure. Ensure the rephrased sentence is neutral and professional.|\\n\\n### Instruction:\\nRephrase the sentence to remove linguistic bias while retaining its original meaning and structure. Ensure the rephrased sentence is neutral and professional.|\\n\\n### Input:\\nBiased sentence: On Tuesday, President Donald Trump will give an Oval Office address on the government shutdown he created, during which he presumably will lie through his teeth to defend his highly unpopular immigration agenda.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "biasedsent = \"On Tuesday, President Donald Trump will give an Oval Office address on the government shutdown he created, during which he presumably will lie through his teeth to defend his highly unpopular immigration agenda.\"\n",
        "biasedwords = ['unpopular', 'lie', 'through', 'his', 'teeth', 'highly']\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Rephrase the sentence to remove linguistic bias while retaining its original meaning and structure. Ensure the rephrased sentence is neutral and professional.|\", # instruction\n",
        "        \"Biased sentence: \" + str(biasedsent) + \"| Biased Words: \"+str(biasedwords),\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 128,use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Qwen-2.5"
      ],
      "metadata": {
        "id": "A2tjGtXzSNlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Loading"
      ],
      "metadata": {
        "id": "HjNl8tkoUIDw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412,
          "referenced_widgets": [
            "bf01b1022ed647168746a1829d49d5ee",
            "be8a3d231ac844b5b0ab38f92e4dc064",
            "fdc23edab9ee460fba54393d35e12ebc",
            "73a8619c9e104a4794356801d7479f59",
            "22a8480b6eb74690a7fc34c88cdbcdf3",
            "48a21921a0ad4c83b1f3798bf31269c7",
            "f18314b81a494720a4f44092259f82b6",
            "50b41cc7dd2c4c669489194f4c214049",
            "e38d6cd6afb64ad9ba7576d6a04c64c5",
            "480f0939c54f46e7b9701a000bf04493",
            "4d0eea2cd03b489688000e79ae9556ab",
            "7c445bbe052b4bc2b092e3fefcf55bf9",
            "98256488f312489a88623deb282219f8",
            "8e6e08464a584656b5b215a1330d9c3a",
            "dd5e504ddce845fb8186d1cfb1eb972c",
            "0e341325e9134998985890393b71d973",
            "7cbee41657e1450cb1df38cabbb4e581",
            "7cfc793cf46f4a95adf20ae321025735",
            "229daa3fce4e428fbbf26aa7cecee61e",
            "066997da5e7d4b92990bc7c33e979246",
            "7332f009941d4ae1af700f022057d033",
            "47ac1fc2b98b412a93dd9204f2fbf327",
            "2a8f908e4583441286f8727581ba84e9",
            "c37cb65117e64f3e8ddad839303ca1d1",
            "6becd869153c40f6a558b812614b6971",
            "b59b13244fd340d389ea4d0d5d2ce668",
            "1c729fa97eb04975940a5d33720f75af",
            "3c44f3716c2a43a6860373284913db6d",
            "91599a19b0aa4df1afb9747c6dd3059f",
            "1e62e675a2cb42de84b537b9876d0d64",
            "ea71cc92499f46329b4efc2101a5107f",
            "57820e1375d5499a981994303ae33eb5",
            "795c0995e0ce429daa12d90271c6c134",
            "8692faefa35847aa8a8e068e9ec22e34",
            "c4301c07a47f4c95a2b940bcd1cfd2ea",
            "d2b8fc5756d64863ab255d99095e946f",
            "04094c4da5874eafaaf639b021395fe2",
            "2c5a82ccb84d4b7494982fe4bee64654",
            "bbaaf37dab9a461bb3f26b216bd6bd35",
            "7644378b23a14b6dba28019bd7c137e9",
            "bdfc557c83644e2a98c52ca635ef1e4e",
            "0803fe5bd2bd475da64660c0a73baaab",
            "1eb8974e97ce4f0bbdbaefcc91cab037",
            "5f07405f469c4d79b093075aeea03f82",
            "6e966369c61e488481f977900e6c325c",
            "2eede4f926764031b795e758598d58bb",
            "4fbf16957b7f470ba865ff6284d7cbba",
            "69fd4a6dbe174325a5c9a943502cd64b",
            "8563f910a53d4ae9b926ef8bed5f9f0c",
            "9e2f6aa93d034585a0ec304f11785bf8",
            "fbced546c4d1449d855e7ca8cd7d1950",
            "10f5461e809245ddb745e7fd60bad7e6",
            "5ef67a68fdf24497b32386cbc140f905",
            "9949f7a73ea048dabe6d2ccd08f1d6d3",
            "21aac72e14834387abfba4b2680124ce",
            "ddffbcfed93a48c5bc59fcae3c3c3fea",
            "3feec474e47f409681cf688e56212c98",
            "9bf9a90ff714448485a177975b36b4e2",
            "f83cec120c8c4ea6a06f6dbc94b85f87",
            "821a39b3d72247d4b3759b916e8673b7",
            "f44925ef2aa84600ab428a0911426f10",
            "8b2cafbbe8f64b128cac0fb3ac43ffb4",
            "5151c1e69c624f47b31665ea7b5080e0",
            "b41c78dc2b6a4c5982b6c9635f96b091",
            "bac6c1485b0d43978bb8a4ffd75388cf",
            "10683125b52d4059bf3745d1dc8e23b9",
            "82967871f1a142ce9bd08f9f1198a799",
            "ecaf45997f5849efa9853410aafe60e5",
            "12fbf7f845cb425b8e2d57c61a9d05a7",
            "c0e6ffaca864406aac5598e10429c158",
            "be2efd3750344546a0200e275e7aa8ca",
            "6293b53a8ae344919042097a182224d6",
            "1c39ba78d21a4b259f11cbde6fe896b9",
            "0385114649c846e99be8e606dbd78f23",
            "6738917d0f7d49a085e366b2ba634281",
            "8a4872da34d847a7af29fa99948c572f",
            "ba98bd2b19134d958bdc5fce869cfa7e",
            "65552d54f4534b25a712b2a7f2cd4d4b",
            "4691c1a675794a75a250d6cef1b18d06",
            "89f95489a1f5407bace58f27724917fd",
            "6e0f48d242ad40d6a38e9fe87f36e0b0",
            "58205311e33840468a0e97e032638dee",
            "6e3e586661244cc889d5e2f072b5df8b",
            "4dec8bf36fad4b90ba94787c40c060e5",
            "329f729b2d214cbc9fd09e2fa30421aa",
            "e9aa4a0e3fc64cd29b1966d09d4c11f4",
            "f1547bfb8db54b60909374deeb2e7e5b",
            "adb8f90ab42145ac9eca17afbe9b8a9d"
          ]
        },
        "outputId": "85e1ac67-e902-4978-c4c6-321a7078eba5",
        "id": "ZbyQbZckSbh7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.1.5: Fast Qwen2 patching. Transformers: 4.47.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.55G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf01b1022ed647168746a1829d49d5ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c445bbe052b4bc2b092e3fefcf55bf9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/4.87k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a8f908e4583441286f8727581ba84e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8692faefa35847aa8a8e068e9ec22e34"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e966369c61e488481f977900e6c325c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/632 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ddffbcfed93a48c5bc59fcae3c3c3fea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82967871f1a142ce9bd08f9f1198a799"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65552d54f4534b25a712b2a7f2cd4d4b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen2.5-7B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e457100-d0d5-4ed9-c27f-a8b3a12f9eee",
        "id": "LOGOWW5OSx2B"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.1.5 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data prep: Same as TinyLLaMA - Alpaca Format"
      ],
      "metadata": {
        "id": "lNqWyhE0Uf0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "yZUC96vqUBcV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db_HvkbDTATV"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = tok_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 4, # Set this for 1 full training run.\n",
        "        # max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8a594544-9131-4b84-f0c1-ee44d7abf1bf",
        "id": "lf44GsjoTJQP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 372 | Num Epochs = 4\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 184\n",
            " \"-____-\"     Number of trainable parameters = 40,370,176\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='184' max='184' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [184/184 13:02, Epoch 3/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.450200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.070300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.243000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.281300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.991700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.993900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.728600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.739500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.662100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.554700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.521300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.655100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.538000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.472600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.628200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.465100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.570400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.486800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.624500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.667400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.230700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.404100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.548700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.650900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.617500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.471000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.578300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.512500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.386900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.413700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.625500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.293300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.437400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.328600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.423900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.328600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.503900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.496500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.689900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.353500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.261500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.344300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.496400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.477000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.600500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.597400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.550300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.502900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.301400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.334300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.422900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.324600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.337800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.444300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.370900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.306500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.290800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.371100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.375400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.506400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>1.343100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>1.306000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.408100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>1.451100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>1.336800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>1.163900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>1.065800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>1.228500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>1.343800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.290800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>1.543400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>1.211800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>1.396400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>1.217100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.149000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>1.419200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>1.317400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>1.353000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>1.210000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>1.227200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>1.181700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>1.370900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.359900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>1.351500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>1.399700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>1.328100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>1.237000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>1.418500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.374800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>1.197400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>1.349000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>1.255900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>1.230700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>1.265300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>1.137700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.998600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>1.154500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>1.178300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.275800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>1.002200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>1.106800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.897600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.939500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>1.067400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.980300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>1.054500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>1.001800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>1.156400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.982800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>1.083900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.857100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>1.002600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.939800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.845400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.928700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.987900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>1.052000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.878800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.927100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>1.010500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.895300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.990600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>1.071200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.841500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.938500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.928800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>1.041400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>1.109800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.784400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.866600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>1.135000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.994300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>0.993800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>1.022200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>1.223700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.874500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.895900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>0.935000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.964800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.918700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.661400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.638700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.710600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.736300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.670500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.659700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.639600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.619000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.661100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>0.768300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.745300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>0.681300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.670300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.876900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.733700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>0.625300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>0.610400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>0.716600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.729200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>0.824000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>0.664000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>0.516900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>0.723700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.657400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>0.661800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>0.744400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.592900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>0.628300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.762300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.636100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.647800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>0.664600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>0.514800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.690100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>0.607900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>0.664400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>0.679300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>0.714000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.450600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>0.574200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.626000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>0.713500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>0.538100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "ktEaZJFhT9x_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47651237-477c-4aea-8ffd-edb4c1ccdc9d",
        "id": "neWylYC2TO60"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nYou are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.\\n\\n### Input:\\nThe biased sentence is: On Tuesday, President Donald Trump will give an Oval Office address on the government shutdown he created, during which he presumably will lie through his teeth to defend his highly unpopular immigration agenda.| The words identified to be carrying bias in the sentence are: ['unpopular', 'lie', 'through', 'his', 'teeth', 'highly']\\n\\n### Response:\\nThe administrationâ€™s response was to present a false narrative about a crisis that didnâ€™t exist, and to use the courts to defend it.<|endoftext|>\"]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "biasedsent = \"On Tuesday, President Donald Trump will give an Oval Office address on the government shutdown he created, during which he presumably will lie through his teeth to defend his highly unpopular immigration agenda.\"\n",
        "biasedwords = ['unpopular', 'lie', 'through', 'his', 'teeth', 'highly']\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"You are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.\",\n",
        "        \"The biased sentence is: \" + str(biasedsent) + \"| The words identified to be carrying bias in the sentence are: \"+str(biasedwords),\n",
        "\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma-2"
      ],
      "metadata": {
        "id": "fxi9MykgWabz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Loading"
      ],
      "metadata": {
        "id": "1SYvZmHNWcxx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348,
          "referenced_widgets": [
            "8108a8d08ac843ef8364857a0e0c5873",
            "4425ae4662e5499a88de85a68f854855",
            "f7453d2dbe90438a866d429ee91cfe73",
            "8af8049ace634107901fbc8a3712f9a8",
            "4842a3049d434c369d6a2b2101dbab86",
            "6de64e3910fa4fc7a2fe7540d3b31964",
            "ae3da03720a646f99280ea101903f7b5",
            "08c1b030b48e4862ae37201ada354088",
            "15a86d519b1f458c8d35d1c303105db7",
            "2a44ecd686b14b86b86b11a86e4b73ff",
            "3ff9d4ec683e47939796725581eea169",
            "cdae019736a04618a95ee4ab4f532a16",
            "4a7b7225f6e64ba89515ab3c487b8034",
            "4ece28420b334f70a2ebe0762c36e104",
            "7f1d246d9e3c4e178d757518b75d425e",
            "70c60a57e8cd41929052e6dccd44c11b",
            "cbda62624d44434f9592d79e98c07ee8",
            "d068365491ea485ab30c42489b0d6ba4",
            "d1c7ab64c6c440a2aab64be37ba548ef",
            "dd1a46e8617648b7a0acf74160c782cb",
            "6381595aafe042d3a927e5ba346c1440",
            "664823500507477f9e1393007385d92b",
            "62cc582daaae48dc8b91febe085a3135",
            "240ae33c87a743d6881b99ffa8d0891e",
            "b1f4c57d1b2248fa8e72c54a42ca3b03",
            "372b3ca0425f4f9692fb916d199e6689",
            "b043e5b3a2154120802bafad47d95811",
            "4136b27ec89e48c7bf830572f771ce5f",
            "63ca7c6c058b4aa0a4111be4aae3a946",
            "fcf755eb42ef43cd93bed425f309c87c",
            "e07d553ff6f94a6d980d4ccc7cb5c192",
            "49445ff3ed494ef3a75dec0661156153",
            "47b06157f8634459a3f28aac43b1ba5b",
            "13e069ef2d154db9a745202a1a83a4d0",
            "f188ba1869b24c518efe22dca55c5ddb",
            "b1fa2a27d7894503823075bb194b3149",
            "4f9374ab55b84a1caaacc33568da0987",
            "fddc67fbc35e45509663de3185f3d5d3",
            "427f80c367b0438e9d4459753307a58f",
            "642e8ab2c7194e0e88b9f63b4523f82f",
            "4079c7b86ee149a3ab1cfc44deb64dfc",
            "768a092988e5484593f339f0ffb950e9",
            "179b035ea8ed4fca9f09c83bbe3ea35f",
            "6d3bb459e3904031b1c8a5c225067d80",
            "bd4709139a0b4e16b1130e5a20fded93",
            "67cdbc98bee441d689fb6aee2f5da9a7",
            "c6a4b6904a564e438d6da0e9003c57e1",
            "4d685a6999424dc28ea0412ab3b22c10",
            "cf627efb0ee649afb55ce41ee92b2f39",
            "38788ec34ffc48c3b361f3ce741e8d40",
            "7cb684f8bc0744248756d1b5d4290a3b",
            "79520b7a040a4a1a87004f5ac58a1c57",
            "e8b4b0b024004235bbd479589ade07b0",
            "b206a67620dc4254852186be8ebd51f4",
            "f692c75f866c4b95a72761143be77603",
            "a84f4be7309e43ceafd875933e527248",
            "d903b5040e3e457fb157aa978f7b30c9",
            "9aa39b9911cf4a478981a45853eea03a",
            "e11cfdf995cd4881aba6123094699012",
            "45a7d4d1fad240e58e3c460bd80a06ca",
            "5a9202d3d6ba40f390a8abc1e4c8f1d7",
            "6121639732ba4670a61f93e273a190ff",
            "0727d03730244b56900d23a66acd3e05",
            "8526aebdfd604f76b23845f7d3414370",
            "217b81846807401b821bac27c71b8fe0",
            "226690be3f4e40179d278cae05677a7d"
          ]
        },
        "outputId": "b15f7afb-b953-4009-962c-d67435735b0e",
        "id": "JQBEPVBzWvpA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.1.5: Fast Gemma2 patching. Transformers: 4.47.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/6.13G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8108a8d08ac843ef8364857a0e0c5873"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdae019736a04618a95ee4ab4f532a16"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62cc582daaae48dc8b91febe085a3135"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13e069ef2d154db9a745202a1a83a4d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd4709139a0b4e16b1130e5a20fded93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a84f4be7309e43ceafd875933e527248"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/gemma-2-9b\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7488a2c4-0d1a-4315-fe93-d7f44128f78e",
        "id": "MW_xI8b9W2Z4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.1.5 patched 42 layers with 42 QKV layers, 42 O layers and 42 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Prep: Alpaca Prompt Format as used in TinyLLaMa"
      ],
      "metadata": {
        "id": "qs9mg3R_W7sD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "MDPbFFDoXGD2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiPIjBfcXCKs"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = tok_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 4,\n",
        "        # max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2091b5f4-44e1-4b90-8f0c-a2abb8977ad9",
        "collapsed": true,
        "id": "2EExvLZFXPEk"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 372 | Num Epochs = 4\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 184\n",
            " \"-____-\"     Number of trainable parameters = 54,018,048\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='86' max='184' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 86/184 07:37 < 08:53, 0.18 it/s, Epoch 1.82/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.558900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.168600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.232900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.956100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.652400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.302600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.760200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.726600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.572900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.407400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.372100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.502900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.355600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.324000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.468200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.322600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.398100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.330500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.438500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.512400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.088800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.269700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.384300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.468400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.480500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.353200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.415000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.288400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.224300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.298200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.411500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.170100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.340800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.211500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.260900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.190500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.351900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.300500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.516600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.205900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.153600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.220700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.321000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.304600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.415000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.424900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.399800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.257100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.055100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.055100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.173300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.128100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.113800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.185700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.110700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.073200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.037900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.097500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.161700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.226600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>1.073500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>1.047900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.074400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>1.110200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>1.106200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.920100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.862500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.987700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>1.069100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.929400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>1.170600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>1.028200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>1.161200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.990800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.933900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>1.172700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>1.010200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>1.054000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.969800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.908600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>1.031600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>1.051700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>1.095000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.103600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-3d62c575fcfd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/tokenizer_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mfloating_point_ops\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4500\u001b[0m         \"\"\"\n\u001b[1;32m   4501\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"floating_point_ops\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4502\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4504\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfloating_point_ops\u001b[0;34m(self, input_dict, exclude_embeddings)\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \"\"\"\n\u001b[1;32m   1280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m6\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexclude_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mnum_parameters\u001b[0;34m(self, only_trainable, exclude_embeddings)\u001b[0m\n\u001b[1;32m   1199\u001b[0m                 \u001b[0;34mf\"{name}.weight\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             ]\n\u001b[0;32m-> 1201\u001b[0;31m             total_parameters = [\n\u001b[0m\u001b[1;32m   1202\u001b[0m                 \u001b[0mparameter\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membedding_param_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1199\u001b[0m                 \u001b[0;34mf\"{name}.weight\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             ]\n\u001b[0;32m-> 1201\u001b[0;31m             total_parameters = [\n\u001b[0m\u001b[1;32m   1202\u001b[0m                 \u001b[0mparameter\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membedding_param_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_parameters\u001b[0;34m(self, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2664\u001b[0m             \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_duplicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m         )\n\u001b[0;32m-> 2666\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_named_members\u001b[0;34m(self, get_members_fn, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2599\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2600\u001b[0m         )\n\u001b[0;32m-> 2601\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2602\u001b[0m             \u001b[0mmembers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2821\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2822\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2823\u001b[0;31m                 yield from module.named_modules(\n\u001b[0m\u001b[1;32m   2824\u001b[0m                     \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2825\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2821\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2822\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2823\u001b[0;31m                 yield from module.named_modules(\n\u001b[0m\u001b[1;32m   2824\u001b[0m                     \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2825\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2821\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2822\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2823\u001b[0;31m                 yield from module.named_modules(\n\u001b[0m\u001b[1;32m   2824\u001b[0m                     \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2825\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2821\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2822\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2823\u001b[0;31m                 yield from module.named_modules(\n\u001b[0m\u001b[1;32m   2824\u001b[0m                     \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2825\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2821\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2822\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2823\u001b[0;31m                 yield from module.named_modules(\n\u001b[0m\u001b[1;32m   2824\u001b[0m                     \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2825\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2821\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2822\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2823\u001b[0;31m                 yield from module.named_modules(\n\u001b[0m\u001b[1;32m   2824\u001b[0m                     \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2825\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2821\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2822\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2823\u001b[0;31m                 yield from module.named_modules(\n\u001b[0m\u001b[1;32m   2824\u001b[0m                     \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2825\u001b[0m                 )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer_stats = trainer.train() # interrupted training as loss did not seem to be improving"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "TVJzoB_eXdEM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78c1dda3-8657-4600-df87-12917ece4681",
        "id": "fE_hSpU4Xfq0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"<bos>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nYou are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.\\n\\n### Input:\\nThe biased sentence is: The explosion of the Hispanic population has long-term job prospect consequences as well: Both legal and illegal aliens will occupy 75 percent of new American jobs in as little as five years.| The words identified to be carrying bias in the sentence are: ['explosion']\\n\\n### Response:\\nThe fact that the Hispanic population is growing rapidly is not a reason to oppose immigration.<eos>\"]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "biasedsent = \"The explosion of the Hispanic population has long-term job prospect consequences as well: Both legal and illegal aliens will occupy 75 percent of new American jobs in as little as five years.\"\n",
        "biasedwords = ['explosion']\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"You are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.\",\n",
        "        \"The biased sentence is: \" + str(biasedsent) + \"| The words identified to be carrying bias in the sentence are: \"+str(biasedwords),\n",
        "\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLaMa-3.2\n"
      ],
      "metadata": {
        "id": "XLwFx-J1TRe7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Loading"
      ],
      "metadata": {
        "id": "PBbuTbEyT6Vt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTH3vafpdVtG",
        "outputId": "9b80f23d-e1ae-4ea4-e326-97df7f180073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316,
          "referenced_widgets": [
            "12368a64849940ec8a8f2543e08b4e1f",
            "2eb123fe08ba4c7b85b5430374275eca",
            "44647571ac124615985125f602d8c17e",
            "c834cb9a34514c2cbaba00f225ea28a0",
            "d195704094ef45f9ac38dd0be4653b7a",
            "d9552922e1f742d6b4adfa05ec4a0cf8",
            "8559b5f816c24bba9d8c7d61d2836f26",
            "2c6469421490402283f039d9b1ce4e7e",
            "2f8163341ad042d19320112e1d9694ed",
            "c170ec79fdc3497d965fdb32aa1b649f",
            "720581751cd347498a2a4cb7d9037dab",
            "0f3a2ecc7a2b4f758568929aa9a2e48f",
            "e037b22459ab43fcb4c4579472b7d80f",
            "35f87e38a90242d9b5abdb13bf6005d4",
            "9552ee65c1e648baab783e34b3d56afa",
            "0485d61809c34e7cb5960831b33f2f99",
            "6572fa7626284ff1a2299b0b1e7188d1",
            "afc4173ab2fc4d99b4491526d04def9a",
            "87001bf0de0d4a64a00f517b823e1838",
            "8cd71b350e7f404ba61aeda2fb2e76c0",
            "6d05ee3638104d729dc95dc229343f57",
            "cdff0b9c03ff42cdbf149dfb31c6e307",
            "a975f8adcd1c42819048a81b48035b5e",
            "cec00f8861944783ad814a4522dd37c4",
            "efe7a3f4f9fc4960b53ee554c1a44d3d",
            "f53f7c491f3e4737a482a2e5af078dcd",
            "1c2eb7f8902d4820927894c2122be7a8",
            "f4b572adef60415787114a2242a8f4f9",
            "0d8fe29dbd3e4db3ad329e7583894c44",
            "efb28ceb25dc4a2fa36eef6ac644c399",
            "b20e5455a2724a13865f74dbe79d480e",
            "7875ca1b17f148ef83b6387237a9beef",
            "062fb02c06014901ad4ff66851b18898",
            "5240c490f6e34018807d44f0937b040e",
            "080bd30dcc9242519609d920fa789634",
            "f9c590bf4a0644739a50ccfb231bd1e3",
            "52c0ee0cf09948e3bc96ecca5bd25b6d",
            "f7d348ff41f240f88a8b50f70caab875",
            "0fa01c251b88489aaeec88833e5037c3",
            "77d725d2e97d47df82ee61bfb1429574",
            "6d348e20317c404db3ed4b534ab475c7",
            "82fcab0ddab945fa8e05208390ff2f20",
            "338aca2d8bb9404b9dacfa4a231292c3",
            "b46971187614498eb2ac3d56c5981d43",
            "5f41ae4ccf9948af80a264b35fad9ad6",
            "af7559b9877a4c07b80fc3e164348f86",
            "dc9ed1a6bdb348609f5e391826809f67",
            "8d9e0a716b8d4aa988a32db57e0ba30d",
            "67883cfc875541c58e887e63721b32ac",
            "4c842699cdec4e48bbb8c4d041d773c3",
            "95bcc64aa0ab47f4ae11f1d5ea29f6b3",
            "e91d882043c94b6bb41ac9876d39fa14",
            "72038422fce745b5828e9adb5b708857",
            "467fed1e67524786af7c741ee572f4d3",
            "0b195a3382764fd489b8530c2f0b7090"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.1.7: Fast Llama patching. Transformers: 4.47.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12368a64849940ec8a8f2543e08b4e1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f3a2ecc7a2b4f758568929aa9a2e48f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a975f8adcd1c42819048a81b48035b5e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5240c490f6e34018807d44f0937b040e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f41ae4ccf9948af80a264b35fad9ad6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95bef0a2-b6b8-4aef-fc43-e62f0ef335e0",
        "id": "eNBBLzUjTuvU"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.1.7 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n"
      ],
      "metadata": {
        "id": "XnwYpVd-T2rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_row(row):\n",
        "    return [\n",
        "        {\"from\": \"system\", \"value\": \"You are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.\"},\n",
        "        {\"from\": \"human\", \"value\": \"The biased sentence is: \" + row[\"biased_sentence\"] + \"| The words identified to be carrying bias in the sentence are: \"+str(row[\"biased_words\"])},\n",
        "        {\"from\": \"gpt\", \"value\": row[\"debiased_sentence\"]}\n",
        "    ]\n",
        "\n",
        "train_conversations = pd.DataFrame()\n",
        "train_conversations[\"conversations\"] = train.apply(format_row, axis=1)\n",
        "\n",
        "val_conversations = pd.DataFrame()\n",
        "val_conversations[\"conversations\"] = val.apply(format_row, axis=1)"
      ],
      "metadata": {
        "id": "kYUQHmBJfYOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "train_dataset = Dataset.from_pandas(train_conversations)\n",
        "val_dataset = Dataset.from_pandas(val_conversations)"
      ],
      "metadata": {
        "id": "UhoOVQIcfZCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset['conversations'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxwsMkBX_Tp2",
        "outputId": "f3fed119-ba55-42cc-d0ad-e35536aa79be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'from': 'system',\n",
              "  'value': \"You are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.\"},\n",
              " {'from': 'human',\n",
              "  'value': \"The biased sentence is: President Donald Trump bragged on Sunday that the Republican Party has never been stronger or more united than it is now, even as his impeachment trial is set to begin in the Senate this week.| The words identified to be carrying bias in the sentence are: ['bragged']\"},\n",
              " {'from': 'gpt',\n",
              "  'value': 'President Donald Trump stated on Sunday that the Republican Party has never been stronger or more united than it is now, even as his impeachment trial is set to begin in the Senate this week.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jtA2kP6U4Mu"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")\n",
        "\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [\n",
        "        tokenizer.apply_chat_template(\n",
        "            convo, tokenize = False, add_generation_prompt = False\n",
        "        )\n",
        "        for convo in convos\n",
        "    ]\n",
        "    return { \"text\" : texts, }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "train_ds = standardize_sharegpt(train_dataset)\n",
        "val_ds = standardize_sharegpt(val_dataset)\n",
        "train_ds = train_ds.map(formatting_prompts_func, batched = True,)\n",
        "val_ds = val_ds.map(formatting_prompts_func, batched = True,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "a5993d95fa7d4f95a5136167ce3afe1a",
            "cf7be329173746fa8d5b61ae0a50df5b",
            "70be733a7eba4aed97d41ef9a9615a29",
            "ee67023ae00947cdb0b6de90da2e7fb9",
            "517f1f1f43bc450bb5beb8ca3abfad5f",
            "36d1bb14e3104d40b243f941a69ad29c",
            "80f635582d154f4fae0ba2b4c19c8637",
            "cb2ad4df8f4c4f18beb1136f3a290e89",
            "f9224ee632374c949f90bca8f74b974c",
            "5077d26aba4647269f5405e3ba5bafdc",
            "97dddc38b479443c96e65d0813b9793e",
            "e539795508e141dcb1aba468ded384cd",
            "5cea72580ab04d239d5e9009f496ca4f",
            "214caf3b649d47be818afdc286c6b0b6",
            "89f457fb5de94129bfa092ebcd11d0e7",
            "2482aa2b17b54c4ba60a4b01ce664595",
            "1a661a8bf82a46478acee055c02ee8cd",
            "04b3b796134f4247beb82d9e419be19c",
            "2f99fe3dae054f31970d8d16c27e1878",
            "dfa2862b7d7a481e9c8be8d11c8472f4",
            "d0a2dec57dc34ff480202ddbe5306f47",
            "6f683fc2e5c5428685ed419239fee1fb",
            "1dfa8c878e0e4043a18f1e2ed62309c4",
            "d9c21fb1f3ab471db629bf524a96b772",
            "c944ef5af02a43a882bcc50ad20c3a47",
            "24b65feaffdd4de4a83f0ddcfb60724d",
            "99205b41bf3f4e5da18013fa59124926",
            "11579fc5708347c2a8e62244592ae723",
            "762a2973faec415cbd5fe65b8d5352ca",
            "d804ea08ff884bcab1e60710ca89c917",
            "75c9aa6eb4a04e5bb06b3ffa35823dad",
            "e1fcc456e0414de2bb1551f6fa269321",
            "2d50787857464f719a1b565dcf2fc67e",
            "c93a603760a34f49abcb9778dc22ea0d",
            "ec71699cf11145d99475f677831641bc",
            "e8b8b3e51bd94cb199ddf82457f91cfc",
            "4f9ca727de1f4418b3b9a8c8d61c895b",
            "45ef2c6c88b5407c8889081efb3cb42d",
            "001cd3880c2b44aeb1f962b01b30f0a8",
            "c63e0f76221546809aef464c4f531cf2",
            "be5d787c0b5642d3ad1bcb161056cc12",
            "5302358e4a6b4ff9b886e2a8ab2a7b58",
            "66551e422b0846748041cc0ed0ef7667",
            "58f8bb0583844c40b0e586d3a59a5f7e"
          ]
        },
        "id": "mmcEuT47earQ",
        "outputId": "ab740513-73cb-43ce-ab40-bd4b88a03a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Standardizing format:   0%|          | 0/401 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5993d95fa7d4f95a5136167ce3afe1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Standardizing format:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e539795508e141dcb1aba468ded384cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/401 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1dfa8c878e0e4043a18f1e2ed62309c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c93a603760a34f49abcb9778dc22ea0d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGFzmplrEy9I",
        "outputId": "1889e0c7-7907-474d-9ad9-7b10a13335bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'content': \"You are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.\",\n",
              "  'role': 'system'},\n",
              " {'content': \"The biased sentence is: Much of this health hazard lies at the feet of folks like RFK Jr., so the idea that Trump was doing the anti-vax slow-jam with this quack a week before Inauguration Day understandably put Twitterverse over the edge| The words identified to be carrying bias in the sentence are: ['anti-vax', 'slow-jam', 'quack']\",\n",
              "  'role': 'user'},\n",
              " {'content': 'Much of this health hazard is attributed to individuals like RFK Jr., so the idea that Trump was associating with this figure a week before Inauguration Day understandably caused a strong reaction on Twitter.',\n",
              "  'role': 'assistant'}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "train_ds[5][\"conversations\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfzTdMtvGE6w"
      },
      "source": [
        "And we see how the chat template transformed these conversations.\n",
        "\n",
        "**[Notice]** Llama 3.1 Instruct's default chat template default adds `\"Cutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\"`, so do not be alarmed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "vhXv0xFMGNKE",
        "outputId": "6d2757a4-64d5-4dde-c45c-75fc12257de3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\nYou are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nThe biased sentence is: Much of this health hazard lies at the feet of folks like RFK Jr., so the idea that Trump was doing the anti-vax slow-jam with this quack a week before Inauguration Day understandably put Twitterverse over the edge| The words identified to be carrying bias in the sentence are: ['anti-vax', 'slow-jam', 'quack']<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nMuch of this health hazard is attributed to individuals like RFK Jr., so the idea that Trump was associating with this figure a week before Inauguration Day understandably caused a strong reaction on Twitter.<|eot_id|>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "train_ds[5][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n"
      ],
      "metadata": {
        "id": "uFZPIAerVZaQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "72ca90b4df4d4b89a77d733e8e937b2e",
            "6fef15f6acd64366a82452748f62aed1",
            "958ca7c91432437f956f6bd311597c2f",
            "77c3c1ca6bd445c08dface680395169d",
            "b12859de46e84013bc4a888615681f16",
            "5dcd3149a08c4b89b16d065977fc832a",
            "c68bef9ed5914dcaa21508e1d3549624",
            "9d8086cba43a4c33a41ba5d31d16a5d3",
            "bf93fea8633145ea8adcca7fc8cad6ca",
            "0fea7862906046a4b14f6a5f0260c219",
            "f92e01d43d464164b958aab861d85fb4",
            "74e9ff89067d4b6a9cc426e036058670",
            "e46bc23634e6408580338b92b7514c24",
            "4ea7fcd970b94262807227ddb92e422f",
            "0d003bed48fc4e689bb774b19b9ff08c",
            "edd54983875a4c6b81298933b16c4fdc",
            "c9d825516dcf4db3a065eb76d2510720",
            "71fe60d62de94df989e4467c2ebf1ef7",
            "0d9159a4f75140e79d13b23a6e93a210",
            "b7a392b46329468182261d917159fef3",
            "435093bb7b394571bcfd73dd2e480a9b",
            "00262306c1bd4313bd2a82bb179de9b4"
          ]
        },
        "outputId": "952b13b1-63d3-4c26-d28d-d4be0141fe49",
        "id": "GtEfRSCtVbxG"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/401 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72ca90b4df4d4b89a77d733e8e937b2e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74e9ff89067d4b6a9cc426e036058670"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq, EarlyStoppingCallback\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_ds,\n",
        "    eval_dataset = val_ds,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 4, # Set this for 1 full training run.\n",
        "        # max_steps = 60,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=10,\n",
        "        # learning_rate = 2e-4,\n",
        "        learning_rate = 1e-5, #decrease learning rate\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "        metric_for_best_model = \"eval_loss\",\n",
        "        greater_is_better = False,\n",
        "        load_best_model_at_end = True\n",
        "    ),\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "dc2efb50fb6b4e4181bcb4a4d56fc5f9",
            "d965cd88598d41dc9da6b82b7bfe4e7c",
            "20e44ca38fc74e1887afe2032d50b64b",
            "e528d5160f584a3f87179b300e5473c1",
            "187a2574d89342b9a594433fb958fdd0",
            "3d4815e3c2254a7186af9851f3b24649",
            "89638c780908454c867b3fccdcb78cdf",
            "6e1da68e4615415b95a9563363f949a6",
            "038232ee0b4d4260b99bd2437f53570f",
            "b87decff01ce4376afe1e0d9900923ee",
            "c531d9b9191641e4b94874e80a26528b",
            "1d0e404c74ea4bd2865b874aae9ddd31",
            "160114810b414f339aff5b2c32867d47",
            "b32b1a302ff84a0f952c2d595234afa6",
            "c816523cb88745a89b3bdfad00875068",
            "bb6e59045e40414f99633906f1ed39b3",
            "be158190964942459a4bbd5f554b6f19",
            "51e4decd1db34127a549a8414344f453",
            "9bb90e5edaac4bf98545c56f46e4c0d2",
            "55a3c2853f7e447db6f59768c5d08bd9",
            "4fe9adeceb0e46e98ae812be6c8d1c0d",
            "b47ed1dcfb0c42dda98235ec457a13e3"
          ]
        },
        "id": "juQiExuBG5Bt",
        "outputId": "0a36b31c-0675-47d6-8988-b6e24777cba3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/401 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc2efb50fb6b4e4181bcb4a4d56fc5f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d0e404c74ea4bd2865b874aae9ddd31"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 789
        },
        "outputId": "0d8219c9-fa28-495e-c754-1b6bb1251773",
        "id": "gwADRWaXVm32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 401 | Num Epochs = 4\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 200\n",
            " \"-____-\"     Number of trainable parameters = 24,313,856\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 13:15, Epoch 3/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.694800</td>\n",
              "      <td>0.963296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.940800</td>\n",
              "      <td>0.812330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.560600</td>\n",
              "      <td>0.694633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.813500</td>\n",
              "      <td>0.612246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.824300</td>\n",
              "      <td>0.561006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.296800</td>\n",
              "      <td>0.535125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.478000</td>\n",
              "      <td>0.522887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.307800</td>\n",
              "      <td>0.514845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.390900</td>\n",
              "      <td>0.509212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.573500</td>\n",
              "      <td>0.505296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.325500</td>\n",
              "      <td>0.501949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.303400</td>\n",
              "      <td>0.499420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.821600</td>\n",
              "      <td>0.497313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.234700</td>\n",
              "      <td>0.495132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.480900</td>\n",
              "      <td>0.493021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.794800</td>\n",
              "      <td>0.491768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.326000</td>\n",
              "      <td>0.490713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.347200</td>\n",
              "      <td>0.489835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.195700</td>\n",
              "      <td>0.489458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.505500</td>\n",
              "      <td>0.489272</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "79-tW9kXVtSa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a59663c-f89c-4d2a-a786-f5b06e5eac14",
        "id": "eucyy4BQVsGp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\nYou are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nThe biased sentence is: Even Richard Nixon, to whom Trump likes to compares himself to, was occasionally willing to venture out into crowds of protesters to talk to them. Trump, however, is such a wuss that he made his cadre of protectors tear-gas people rather than subject himself to a crowdâ€™s insults. Far from proving he was a tough guy, Trumpâ€™s actions only led to another round of taunting about his wimpiness.| The words identified to be carrying bias in the sentence are: ['wuss', 'wimpiness']<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nEven Richard Nixon, to whom Trump sometimes compares himself to, was occasionally willing to venture out into crowds of protesters to talk to them. Trump, however, chose to prioritize his security rather than confronting the crowdâ€™s insults. This decision did little to dispel perceptions of his toughness, as it ultimately led to more criticism about his handling of the situation.<|eot_id|>\"]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "biased_sent = \"Even Richard Nixon, to whom Trump likes to compares himself to, was occasionally willing to venture out into crowds of protesters to talk to them. Trump, however, is such a wuss that he made his cadre of protectors tear-gas people rather than subject himself to a crowdâ€™s insults. Far from proving he was a tough guy, Trumpâ€™s actions only led to another round of taunting about his wimpiness.\"\n",
        "biased_words = ['wuss', 'wimpiness']\n",
        "\n",
        "messages = [\n",
        "    {'content': \"You are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.\",\n",
        "  'role': 'system'},\n",
        " {'content': \"The biased sentence is: \" + biased_sent + \"| The words identified to be carrying bias in the sentence are: \"+str(biased_words),\n",
        "  'role': 'user'}\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 128, use_cache = True,\n",
        "                         temperature = 1, min_p = 0.1)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference on Test"
      ],
      "metadata": {
        "id": "-P8uVWSSV1a7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save model outputs: use to measure model performance through text generation metrics"
      ],
      "metadata": {
        "id": "OJQtFHbTWTEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_debiased_sentences(biased_sent, biased_words, tokenizer):\n",
        "    tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "    messages = [\n",
        "        {'content': \"You are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.\",\n",
        "      'role': 'system'},\n",
        "    {'content': \"The biased sentence is: \" + biased_sent + \"| The words identified to be carrying bias in the sentence are: \"+str(biased_words),\n",
        "      'role': 'user'}\n",
        "    ]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize = True,\n",
        "        add_generation_prompt = True, # Must add for generation\n",
        "        return_tensors = \"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(input_ids = inputs, max_new_tokens = 128, use_cache = True,\n",
        "                            temperature = 1, min_p = 0.1)\n",
        "    text_output = tokenizer.batch_decode(outputs)[0]\n",
        "    text_output = text_output.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")[1].strip()\n",
        "    text_output = text_output.split(\"<|eot_id|>\")[0].strip()\n",
        "    return text_output\n",
        "\n",
        "to_csv = { 'biased_sentence': [], 'gpt_debiased_sentences' : [], 'model_outputs' : [] }\n",
        "\n",
        "for biased_sent, biased_words, debiased_sent in zip(test['biased_sentence'], test['biased_words'], test['debiased_sentence']):\n",
        "    out_sentence = generate_debiased_sentences(biased_sent, biased_words, tokenizer)\n",
        "    to_csv['biased_sentence'].append(biased_sent)\n",
        "    to_csv['gpt_debiased_sentences'].append(debiased_sent)\n",
        "    to_csv['model_outputs'].append(out_sentence)\n",
        "\n",
        "df = pd.DataFrame(to_csv)\n",
        "df.to_csv('/content/drive/Shareddrives/FYP 2024-2025/Phase-2/model-evaluation/llama-3.2-3b.csv', index=False)\n"
      ],
      "metadata": {
        "id": "NLb2UfO6HEo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "v-KSBnnzH20A",
        "outputId": "0ab7ac6e-97ba-4cc5-c420-669489c42d65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     biased_sentence  \\\n",
              "0  The anti-vaccination community holds to an uns...   \n",
              "1  Meanwhile, a pervasive celebrity culture â€” in ...   \n",
              "2  As the world as we know it comes to a quicker-...   \n",
              "3  Coronavirus vaccine and quarantine protesters ...   \n",
              "4  Yet it's merely the tip of the iceberg of war ...   \n",
              "\n",
              "                              gpt_debiased_sentences  \\\n",
              "0  Some groups oppose childhood vaccinations, cla...   \n",
              "1  Meanwhile, a widespread celebrity culture â€” in...   \n",
              "2  As much of Australia faces wildfires that clim...   \n",
              "3  Coronavirus vaccine and quarantine protesters ...   \n",
              "4  Yet it's only the beginning of the issue of wa...   \n",
              "\n",
              "                                       model_outputs  \n",
              "0  The community holding these views believes tha...  \n",
              "1  Meanwhile, celebrity culture â€” in which celebr...  \n",
              "2  As the world as we know it comes to an end, wi...  \n",
              "3  Coronavirus vaccine and quarantine protesters ...  \n",
              "4  Yet it's merely a small part of a larger issue...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dd572e11-0788-47a4-a796-b0b8207cf6e7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>biased_sentence</th>\n",
              "      <th>gpt_debiased_sentences</th>\n",
              "      <th>model_outputs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The anti-vaccination community holds to an uns...</td>\n",
              "      <td>Some groups oppose childhood vaccinations, cla...</td>\n",
              "      <td>The community holding these views believes tha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Meanwhile, a pervasive celebrity culture â€” in ...</td>\n",
              "      <td>Meanwhile, a widespread celebrity culture â€” in...</td>\n",
              "      <td>Meanwhile, celebrity culture â€” in which celebr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>As the world as we know it comes to a quicker-...</td>\n",
              "      <td>As much of Australia faces wildfires that clim...</td>\n",
              "      <td>As the world as we know it comes to an end, wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Coronavirus vaccine and quarantine protesters ...</td>\n",
              "      <td>Coronavirus vaccine and quarantine protesters ...</td>\n",
              "      <td>Coronavirus vaccine and quarantine protesters ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Yet it's merely the tip of the iceberg of war ...</td>\n",
              "      <td>Yet it's only the beginning of the issue of wa...</td>\n",
              "      <td>Yet it's merely a small part of a larger issue...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd572e11-0788-47a4-a796-b0b8207cf6e7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dd572e11-0788-47a4-a796-b0b8207cf6e7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dd572e11-0788-47a4-a796-b0b8207cf6e7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4de5dda7-b40b-46d9-b026-424bd17116f8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4de5dda7-b40b-46d9-b026-424bd17116f8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4de5dda7-b40b-46d9-b026-424bd17116f8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 51,\n  \"fields\": [\n    {\n      \"column\": \"biased_sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 51,\n        \"samples\": [\n          \"After tremendous pressure on the Department of Homeland Security, Miller is advancing his agenda of not just blocking migrants from entering the country illegally, but making life in the U.S. so unbearable for legal immigrants that they leave the country.\",\n          \"Netflix's statements related to leaving Georgia if fetal heartbeat law goes into effect is the company\\u2019s way of firing a warning shot to Georgia and similar states: Conform to our Hollywood ideals and values, or we won\\u2019t give you our business.\",\n          \"Shelby Steele, a senior fellow at Stanford University\\u2019s Hoover Institution said Friday that the contemporary civil rights movement under the banner of \\u201cBlack Lives Matter\\u201d was deeply unserious, catering to an old form of victimization that has accomplished nothing to lift up black people.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gpt_debiased_sentences\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 51,\n        \"samples\": [\n          \"After significant pressure on the Department of Homeland Security, Miller is advancing his efforts to not only block migrants from entering the country illegally, but also to make life in the U.S. difficult for legal immigrants, encouraging them to leave.\",\n          \"Netflix's statements related to leaving Georgia if the fetal heartbeat law goes into effect is the company\\u2019s way of signaling to Georgia and similar states: Adhere to our values, or we won\\u2019t conduct business with you.\",\n          \"Shelby Steele, a senior fellow at Stanford University\\u2019s Hoover Institution, said Friday that the contemporary civil rights movement under the banner of \\u201cBlack Lives Matter\\u201d focuses on longstanding issues but has yet to demonstrate significant progress in improving conditions for black communities.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_outputs\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 51,\n        \"samples\": [\n          \"After increased pressure on the Department of Homeland Security, Miller is advancing his plan to block migrants from entering the country illegally, and making life in the U.S. difficult for legal immigrants who choose to leave the country.\",\n          \"Netflix's statements related to leaving Georgia if fetal heartbeat law goes into effect is the company\\u2019s way of issuing a warning to Georgia and similar states: Meet our Hollywood ideals and values, or we won\\u2019t give you our business.\",\n          \"Shelby Steele, a senior fellow at Stanford University\\u2019s Hoover Institution, said on Friday that the contemporary civil rights movement under the banner of \\u201cBlack Lives Matter\\u201d was seen as unserious, catering to an old form of victimization that has yet to improve the lives of black people.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mistral-v0.3"
      ],
      "metadata": {
        "id": "rwwi5OuWXzEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Loading"
      ],
      "metadata": {
        "id": "F5xtxutJX40M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d42d8ac-b44f-4095-dfa2-3d876a24f8cf",
        "id": "AR-QE_JKYC-3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.1.7: Fast Mistral patching. Transformers: 4.47.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZYJuP18YLIm"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "624zAQmLYUz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_row(row):\n",
        "    return [\n",
        "        {\"from\": \"system\", \"value\": \"You are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.\"},\n",
        "        {\"from\": \"human\", \"value\": \"The biased sentence is: \" + row[\"biased_sentence\"] + \" | The words identified to be carrying bias in the sentence are: \"+str(row[\"biased_words\"])},\n",
        "        {\"from\": \"gpt\", \"value\": row[\"debiased_sentence\"]}\n",
        "    ]\n",
        "\n",
        "# Convert the dataset into the desired format\n",
        "formatted_train_data = [format_row(row) for _, row in train.iterrows()]\n",
        "formatted_test_data = [format_row(row) for _, row in test.iterrows()]\n",
        "formatted_val_data = [format_row(row) for _, row in val.iterrows()]\n",
        "\n",
        "train_ds = pd.DataFrame({\"conversations\": formatted_train_data})\n",
        "test_ds = pd.DataFrame({\"conversations\": formatted_test_data})\n",
        "val_ds = pd.DataFrame({\"conversations\": formatted_val_data})"
      ],
      "metadata": {
        "id": "V3VrnfIbJdiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "hf_train = Dataset.from_pandas(train_ds)\n",
        "hf_test = Dataset.from_pandas(test_ds)\n",
        "hf_val = Dataset.from_pandas(val_ds)"
      ],
      "metadata": {
        "id": "XMuuoOzaJo5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "737e4435148c4c3aac8a58db47e4ff19",
            "eb99306a822f4574bd63d9d26fcb8536",
            "37a562985d6149728404e85e44ea07aa",
            "3c0a38e93a4e4658ac9d7ab53c6f8465",
            "3a6835aed5844695aeb128eac992339a",
            "0b4191074cae4d7dac115b68afaad947",
            "113b769a3c28475c8712961eb9556ad0",
            "000ab8f89c514a68ab35fb2f471fc945",
            "2fb40de632a64cd9963576539b6731bb",
            "c45ab3005db843c8a929543baf680f1d",
            "b4f521aad50d47fa9496fe10f0fa535b",
            "f24f0e31599c43b7ac4357a361762388",
            "24e72a782c514aca834fc21abdb19015",
            "fb51a2dc698f460ab72787f408db0743",
            "2af8c00f87bf46fdaa861661e94aad82",
            "3db4089072b84dc292d40244306abb07",
            "26489a43e8f04d96b57551dffb8e2740",
            "ed1b6001da844b0fbf61808e6c0ca7b4",
            "f7586c9a6d9c4756a218a2e416381e00",
            "14f20a4b33f24afc89b41993f6cee104",
            "f43abe4d98204a76b22f7add985ad315",
            "4f534a55187d400fb0765f84b899780b",
            "11bf1242145141ed84e0d1c332090eb0",
            "01d47c7d6cb8460282dd59f081b82e3d",
            "5451b2121c704ab3a485ee92961c3f67",
            "84ba95943425410da164fe230f3e4fbd",
            "ede164ff3cbe45fcbaf6df15527181f5",
            "68c954e757a248159dea9b5b15c24cf8",
            "1e4e00295eda42bc98dd3c878d25c52d",
            "e9cae84658024945bd9b7fd2a6a12d20",
            "65e348bc455140999c7132d2750cd47e",
            "e44473125346442c993d168591aaba75",
            "076a5aa4a0374926b3c7dbdc50a3840a"
          ]
        },
        "outputId": "ac661678-dbd7-4e37-fe6d-31cf6484a113",
        "id": "2nBKh2btYhm_"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/401 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "737e4435148c4c3aac8a58db47e4ff19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f24f0e31599c43b7ac4357a361762388"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11bf1242145141ed84e0d1c332090eb0"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\", \"system\": \"system\"}, # ShareGPT style\n",
        "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "# from datasets import load_dataset\n",
        "# dataset = load_dataset(\"philschmid/guanaco-sharegpt-style\", split = \"train\")\n",
        "train_dataset = hf_train.map(formatting_prompts_func, batched = True,)\n",
        "val_dataset = hf_val.map(formatting_prompts_func, batched = True,)\n",
        "test_dataset = hf_test.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GSuKSSbpYKq",
        "outputId": "d5441014-2784-42ad-9622-5b433dfa5b88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'from': 'system',\n",
              "  'value': \"You are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.\"},\n",
              " {'from': 'human',\n",
              "  'value': \"The biased sentence is: Drowned out by the algorithm: Vaccination advocates struggle to be heard online | The words identified to be carrying bias in the sentence are: ['Drowned', 'out', 'struggle']\"},\n",
              " {'from': 'gpt',\n",
              "  'value': 'Overpowered by the algorithm: Vaccination advocates face challenges in gaining attention online.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "dataset[5][\"conversations\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5iEWrUkevpE",
        "outputId": "05c4eea4-281f-4592-efdf-999c732a22c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|im_start|>system\n",
            "You are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.<|im_end|>\n",
            "<|im_start|>user\n",
            "The biased sentence is: Drowned out by the algorithm: Vaccination advocates struggle to be heard online | The words identified to be carrying bias in the sentence are: ['Drowned', 'out', 'struggle']<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Overpowered by the algorithm: Vaccination advocates face challenges in gaining attention online.<|im_end|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(dataset[5][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "Wsk64YKJYpsr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "03729e0178194ada88d1d677cbf5e659",
            "2411ca6871bc4711b11632d7ffd40ea4",
            "f5f9f0a9b97b4dd7a75898f1647b03c9",
            "ce3f8c578a2c41eda8e1db5a9e1ae2e0",
            "c88685c7956549c9a047efdff7028239",
            "ad594792d1674d5e86b2a04335516ed4",
            "36bbc745b5b24d43b55b7c82b10cba91",
            "0f59957cdca740769d7c226e723571bf",
            "5c5137a939284b6193e4e45c3fef3e78",
            "ff0b5d75013945a9aa0f8aa2266e8242",
            "ad2fae64d85747b8a76c9ac2b5e1505f",
            "45479ccaea374d069267b146169a4923",
            "0261da43584e4cbe85b11413647b5ec7",
            "c8febf216a214445b1213ae01bb8e7ae",
            "08c5efa46b3b4e42b16350ecfbac5117",
            "4136d31a95ac4e15b7c4f27f853e4770",
            "beaea4b69d924f188384011bf911c0de",
            "5f7242a7c69f448bbf1b14f1dd735373",
            "6277e2ee24804455987504db6c99144e",
            "9c9eb137457042659ec6e7cefe9d661f",
            "20c8554e0a234a61bedf4d0b901c712a",
            "dd7d5e5b9baa4aba9e04408fb0587a9e"
          ]
        },
        "outputId": "ce688e7d-4e54-4e29-f106-202e9587029b",
        "id": "EZu5uswfYtR7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/401 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03729e0178194ada88d1d677cbf5e659"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45479ccaea374d069267b146169a4923"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, EarlyStoppingCallback\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3)\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = val_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        save_on_each_node=True,\n",
        "        save_steps=10,\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 4,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=10,\n",
        "        # max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "        metric_for_best_model = \"eval_loss\",\n",
        "        greater_is_better = False,\n",
        "        load_best_model_at_end = True\n",
        "    ),\n",
        "     callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "d6b69c39-40a8-4365-8827-fe23c51169b7",
        "id": "_SKmnIhxYwSI"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 401 | Num Epochs = 4\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 200\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='80' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 80/200 11:33 < 17:46, 0.11 it/s, Epoch 1/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.581400</td>\n",
              "      <td>0.592407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.560700</td>\n",
              "      <td>0.563869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.522100</td>\n",
              "      <td>0.544289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.566400</td>\n",
              "      <td>0.528344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.504300</td>\n",
              "      <td>0.527118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.457300</td>\n",
              "      <td>0.535503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.465000</td>\n",
              "      <td>0.532511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.357100</td>\n",
              "      <td>0.542369</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "Qj13Ty2DY19D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf754777-65f0-454f-fe35-4faea8e322af",
        "id": "430SeEA0Y3g6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Will map <|im_end|> to EOS = <|im_end|>.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"<|im_start|>system\\nYou are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.<|im_end|>\\n<|im_start|>system\\nThe biased sentence is: And sure, Republicans are stupid enough to go out and get infected, doing the virusâ€™ dirty work for it. But the economy isnâ€™t going anywhere as long as most people, who are sane and smart, refuse to put themselves in danger for a movie or haircut.| The words identified to be carrying bias in the sentence are: ['stupid', 'sane', 'smart']<|im_end|>\\n<|im_start|>assistant\\nAnd sure, Republicans are making choices that put them at risk of infection, contributing to the spread of the virus. But the economy isnâ€™t going anywhere as long as most people, who are cautious and careful, refuse to put themselves in danger for a movie or haircut.<|im_end|>\"]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\", \"system\": \"system\"}, # ShareGPT style\n",
        "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "biased_sent = \"And sure, Republicans are stupid enough to go out and get infected, doing the virusâ€™ dirty work for it. But the economy isnâ€™t going anywhere as long as most people, who are sane and smart, refuse to put themselves in danger for a movie or haircut.\"\n",
        "biased_words = ['stupid', 'sane', 'smart']\n",
        "\n",
        "messages = [\n",
        "    {'value': \"You are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.\",\n",
        "  'from': 'system'},\n",
        "    {'value': \"The biased sentence is: \" + biased_sent + \"| The words identified to be carrying bias in the sentence are: \"+str(biased_words),\n",
        "      'from': 'user'}\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 128, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference on Test"
      ],
      "metadata": {
        "id": "Y6RM3NQkY_ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "# Function to perform inference on a single row\n",
        "def run_inference(test, model, tokenizer, csv_file_path):\n",
        "    # Define the CSV header\n",
        "    header = [\"biased_sentence\", \"debiased_sentence\", \"model_output\"]\n",
        "\n",
        "    # Open the CSV file for writing\n",
        "    with open(csv_file_path, mode='w', newline='') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=header)\n",
        "        writer.writeheader()  # Writing the header to the CSV\n",
        "\n",
        "        # Iterate over each row in the DataFrame\n",
        "        i=0\n",
        "        for _, row in test.iterrows():\n",
        "            print(i)\n",
        "            i=i+1\n",
        "            biased_sent = row[\"biased_sentence\"]\n",
        "            biased_words = row[\"biased_words\"]\n",
        "            debiased_sentence = row[\"debiased_sentence\"]\n",
        "\n",
        "            # Create the messages for the model\n",
        "            messages = [\n",
        "                {'value': \"You are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.\", 'from': 'system'},\n",
        "                {'value': \"The biased sentence is: \" + biased_sent + \"| The words identified to be carrying bias in the sentence are: \" + str(biased_words), 'from': 'user'}\n",
        "            ]\n",
        "\n",
        "            # Tokenize the input messages\n",
        "            inputs = tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                tokenize=True,\n",
        "                add_generation_prompt=True,  # Must add for generation\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(\"cuda\")\n",
        "\n",
        "            # Run inference using the model\n",
        "            outputs = model.generate(input_ids=inputs, max_new_tokens=128, use_cache=True)\n",
        "            output_sentence = tokenizer.batch_decode(outputs)[0]  # Take the first output\n",
        "\n",
        "            # Write the result to the CSV file\n",
        "            writer.writerow({\n",
        "                \"biased_sentence\": biased_sent,\n",
        "                \"debiased_sentence\": debiased_sentence,\n",
        "                \"model_output\": output_sentence  # Use the first generated output\n",
        "            })\n",
        "\n",
        "# Example usage\n",
        "import os\n",
        "\n",
        "csv_file_path = \"/content/drive/Shareddrives/FYP 2024-2025/Phase-2/model-evaluation/mistral.csv\"\n",
        "directory = os.path.dirname(csv_file_path)\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "\n",
        "# Assuming model and tokenizer are already defined and initialized\n",
        "run_inference(test, model, tokenizer, csv_file_path)\n"
      ],
      "metadata": {
        "id": "9ZpTRTzlZBjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file = \"/content/drive/Shareddrives/FYP 2024-2025/Phase-2/model-evaluation/mistral.csv\"\n",
        "import re\n",
        "\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Function to modify model_output by extracting from the first occurrence of \"assistant\" to the following \"<|im_end|>\"\n",
        "def modify_model_output(output):\n",
        "    # Use regular expression to find the part of the output starting from 'assistant' and ending at the next '<|im_end|>'\n",
        "    match = re.search(r\"<\\|im_start\\|>assistant(.*?)<\\|im_end\\|>\", output, re.DOTALL)\n",
        "\n",
        "    if match:\n",
        "        # Extract the text between the markers and modify it (example: you can perform any specific editing here)\n",
        "        extracted_text = match.group(1).strip()  # Group 1 is the content between 'assistant' and '<|im_end|>'\n",
        "        # Example modification: Just returning the extracted portion for now\n",
        "        modified_output = extracted_text\n",
        "    else:\n",
        "        # If no match is found, return the original output\n",
        "        modified_output = output\n",
        "\n",
        "    return modified_output\n",
        "\n",
        "\n",
        "# Apply the modification function to each model_output row\n",
        "df['model_output'] = df['model_output'].apply(modify_model_output)\n",
        "\n",
        "# Save the modified DataFrame back to a CSV\n",
        "modified_csv_file_path = \"/content/drive/Shareddrives/FYP 2024-2025/Phase-2/model-evaluation/mistral_7B.csv\"\n",
        "df.to_csv(modified_csv_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "jHCViuTV9378"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phi-3.5-mini"
      ],
      "metadata": {
        "id": "7t-OT23ZZLib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Loading"
      ],
      "metadata": {
        "id": "mrPG2mT3ZVEN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380,
          "referenced_widgets": [
            "69be24a58010448888f32a26d4de5018",
            "8194c605d0804e9bb63abc22817ff926",
            "960c9b5671e24922802f79ce5d052533",
            "d76dd22248e741ec8382de92ace8da51",
            "cf3be712e8ed42fa9a1dc37bff06380b",
            "bc07a0f2054b4f6d91949deaad2c5a40",
            "cd15ae78709d45f2a5339043128743ff",
            "b9e1004c3d2c49c6b32f5450bc6b5361",
            "735522e66248443f80230816aede6d58",
            "dc1433700683459794d31def48fbe08c",
            "4454d91c0c6d4298b3e03805ce89c5b1",
            "820602d9f5ac4e598046ca61b1b4c931",
            "334340f42939438ea537d4f2eb410441",
            "4136e2d3ae3a453bbec0ffc051c6ad72",
            "c90c8b0857ee41828a4ff8f201cbe80f",
            "c7fd9a50a79b42cfa5196920556aa535",
            "44d34e931e964ce19624dca244bb1675",
            "2db42a1306944279abe243fdfa17a57a",
            "126a544acc0a42b7968a1aa27c2bfcbc",
            "f76aeed13b4c4aa885ce5cb77963f5cc",
            "ebe09333df6f449b92d36700feb9ea93",
            "956cad9ae8ea49868db3c46829f64bcc",
            "294f1807d3a64fe3990348969567168d",
            "f3438170314246a9a3e915087cefad29",
            "7c22e5f9839546a68e32f0c0b9e82014",
            "c27c41cb46d84da2b60c817adb3405f1",
            "894cd9e333eb4fadb965328716654905",
            "d629104ba65b46a89014671048c56ff1",
            "4092ba350ca2404ebce095762c59adcc",
            "57b0a0cffa6f4c768aa9d6bfb745d4ba",
            "da7c5fad47574b99a96da3cbec354f92",
            "f1ec8ea3fc73456085644376119ae91f",
            "02955d1e2adb4b5daa3297516dc28ec1",
            "f0236cbbb6174b9097487fb4e94984dc",
            "22c6b4fc0b674333b61901879b83eee6",
            "8b21eafc9ccd4989b4fbffdd1095b9c1",
            "19f7ea14a9d24b4386edd49481f06c50",
            "732de8a77b3644d88e676a2aeeb0da3d",
            "bba854e60e2644b3965d6600d40fb3f8",
            "6aef79d6fcc440b0ac45735632deb45f",
            "ec7c2c1631df447d99aef9e3e72bdc7b",
            "33baa2d2b7ae46ccb7413b7eb45a968b",
            "63fb54eafa2d42f09bfb8656220f9228",
            "06c4573134034f108333d91f374e9119",
            "1d39ed4f5bdc40b1948ca247647f3d9f",
            "ba82d14b36264a01b080d317f7d2f816",
            "ed1b858c9b4548868adbc4ea309b4cd9",
            "4c6828e5e012480a92a4c999f93ba692",
            "cfc54ddf09704f2ebf8f1a161ebd5544",
            "4a47b3a9371444f6a7a9fe71686688e0",
            "d5b411015fef498b92cc92bb00489551",
            "5b1fba3b2483461589b11eed44749366",
            "8ce3ad99ef2e45aa97d3625bc6f4a9b9",
            "e621df91155d44c39c41089ed49c6e2d",
            "9cefd77d3b1e498c8f402a8d3f414e8d",
            "dce4243724bb434497613ae01fb5c042",
            "59b1b4b062d541c6918e7bdacd548da6",
            "1768ff3d0ce74c76abf632294f96fd4c",
            "e9d55644fb52469b8c95db1f9a816a8c",
            "92ccb1b6baef4604b12db092d2b09127",
            "c29d8e890f9e4580b11c1344f434179b",
            "25b2dce1e08e485c87d530e407916131",
            "868c77251ede481ab22c55488b6d970b",
            "1c08dff2f8aa41c9b9c0e01a3406da0a",
            "7fada4a2164f4cadb018ff7bd370b49c",
            "aec7ea0b16c643ddb74b03146a8569c3",
            "f43d856467594ca7911a6ecb5a43e694",
            "1f57026eba8c434db676a110290219d1",
            "e6512d019f4147bd9213db920900bad4",
            "5217efa0ffbe4c03bb397d84ded97ae8",
            "e23dc453c1df402aae403300842166b7",
            "0ea9514de5ba4203ab5496c6a93a3ca2",
            "d83b313a1ffb40a693a0bb163cc04988",
            "9aabd750564147759839e6087cd2f905",
            "cba69c9a1db241aeac505f37dcd6e92d",
            "37ca32236e4f45438846ea7f21baeeaf",
            "1292a33139fe455e90798bd5dfd4a62f"
          ]
        },
        "outputId": "cc1a3ffd-f214-4ddf-bb41-8017c8310995",
        "id": "Zki7ES5uZcTL"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.1.7: Fast Llama patching. Transformers: 4.47.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.26G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69be24a58010448888f32a26d4de5018"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/140 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "820602d9f5ac4e598046ca61b1b4c931"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/3.37k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "294f1807d3a64fe3990348969567168d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0236cbbb6174b9097487fb4e94984dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d39ed4f5bdc40b1948ca247647f3d9f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dce4243724bb434497613ae01fb5c042"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f43d856467594ca7911a6ecb5a43e694"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Phi-3.5-mini-instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57166350-e159-481d-d0c4-63d2494faba7",
        "id": "vHKxt3rTZoQm"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.1.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "IrX8wHukZsJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instruction : You are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.\n",
        "\n",
        "# Define a function to create a conversational format\n",
        "def format_row(row):\n",
        "    return [\n",
        "        {\"from\": \"system\", \"value\": \"You are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.\"},\n",
        "        {\"from\": \"human\", \"value\": \"The biased sentence is: \" + row[\"biased_sentence\"] + \"| The words identified to be carrying bias in the sentence are: \"+str(row[\"biased_words\"])},\n",
        "        {\"from\": \"gpt\", \"value\": row[\"debiased_sentence\"]}\n",
        "    ]\n",
        "\n",
        "# Convert the dataset into the desired format\n",
        "formatted_data = [format_row(row) for _, row in df.iterrows()]\n",
        "\n",
        "train_conversations = pd.DataFrame()\n",
        "val_conversations = pd.DataFrame()\n",
        "test_conversations = pd.DataFrame()\n",
        "\n",
        "train_conversations[\"conversations\"] = train.apply(format_row, axis=1)\n",
        "val_conversations[\"conversations\"] = val.apply(format_row, axis=1)\n",
        "test_conversations[\"conversations\"] = test.apply(format_row, axis=1)\n"
      ],
      "metadata": {
        "id": "xupBPn2f2h8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "train_ds = Dataset.from_pandas(train_conversations)\n",
        "val_ds = Dataset.from_pandas(val_conversations)\n",
        "test_ds = Dataset.from_pandas(test_conversations)"
      ],
      "metadata": {
        "id": "LDvFFF_P2xUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"phi-3\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\", \"system\" : \"system\"}, # ShareGPT style\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "train_dataset = train_ds.map(formatting_prompts_func, batched = True,)\n",
        "val_dataset = val_ds.map(formatting_prompts_func, batched = True,)\n",
        "test_dataset = test_ds.map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "id": "Edrn7Rxmojtu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "ad1ea8284b8b45729425ea5774e78ca8",
            "1a548ee539994428b54b15ca258d6630",
            "c790d790d1c3447bab75e6afcaa7d922",
            "5cb2d50ac36c4b7da17de3d4835a4bd3",
            "64f3e8f988ed4a74bd290ab35b058b0a",
            "034d36d259a94ebf8dfd6dd34c7a17f5",
            "4c7b51fe026141808957815d2453f992",
            "15b91a0309a54080b4b7b36940dc11aa",
            "05ecc414104f4afb9dae64d149acadd8",
            "d89f6aa163d84966b6598164903cfca8",
            "2c636d77f9934fe29f255fefd92b3e64",
            "2467bff111354bca9dbc8be433631e18",
            "ac5ce93ccb2a431c9a87cf7384e64336",
            "426b544faaae4706a29615cb1633cf86",
            "310066add2f54c62bbbbc17abfd496da",
            "3132a2d44f364c00bc92c9de2b2a8b38",
            "8145b7fca91e4b8babc980f09d9a6e8a",
            "5261bd8ba95a4f3282ac4fbbf4989687",
            "3577c975b2064c3089424f3c34c0e466",
            "503dddbf8cd34b7aba56070312e85415",
            "30dc6ea65a6a471e849f236efc3b8e8c",
            "dd382f98f7bc48ef9d1b280bb20674a3",
            "6f8ff1e6a7cf48988f0c2964c0b3a62f",
            "e0a9f871f0114929bfa580bb9f7f223f",
            "eb0def34a9194195ac93ecedad15dd72",
            "ae26efb30d984ab4819fd4d3c943810a",
            "0b54ad3c78154c4dbed617bd5a3aea2c",
            "044188007e02421092095e2ee654fd80",
            "c9a3d0c181ae49e288930a44056ca4a7",
            "4daa1a67cc6f43d082f9b7a92d5e8b9b",
            "37d0ad20519c4ea2b6f691c4902e574f",
            "cc5d6f1813ea4d338a6285fff88e9479",
            "54a1c0d406b74816ac1f0cb3cca9138f"
          ]
        },
        "outputId": "2653e6a9-9920-4177-c86c-4b7bc3c6a2ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/401 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad1ea8284b8b45729425ea5774e78ca8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2467bff111354bca9dbc8be433631e18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f8ff1e6a7cf48988f0c2964c0b3a62f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[5][\"conversations\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9259fed-d81a-4eb6-e821-8371ccb61cd7",
        "id": "gnm7MGcRZ4o6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'from': 'system',\n",
              "  'value': \"You are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.\"},\n",
              " {'from': 'human',\n",
              "  'value': \"The biased sentence is: Much of this health hazard lies at the feet of folks like RFK Jr., so the idea that Trump was doing the anti-vax slow-jam with this quack a week before Inauguration Day understandably put Twitterverse over the edge| The words identified to be carrying bias in the sentence are: ['anti-vax', 'slow-jam', 'quack']\"},\n",
              " {'from': 'gpt',\n",
              "  'value': 'Much of this health hazard is attributed to individuals like RFK Jr., so the idea that Trump was associating with this figure a week before Inauguration Day understandably caused a strong reaction on Twitter.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[5][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f193f58-746b-45cf-afee-dce48990002b",
        "id": "nzcAJm-xZ7B1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "You are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.<|end|>\n",
            "<|user|>\n",
            "The biased sentence is: Much of this health hazard lies at the feet of folks like RFK Jr., so the idea that Trump was doing the anti-vax slow-jam with this quack a week before Inauguration Day understandably put Twitterverse over the edge| The words identified to be carrying bias in the sentence are: ['anti-vax', 'slow-jam', 'quack']<|end|>\n",
            "<|assistant|>\n",
            "Much of this health hazard is attributed to individuals like RFK Jr., so the idea that Trump was associating with this figure a week before Inauguration Day understandably caused a strong reaction on Twitter.<|end|>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "Ql_jyHrgZ_Xe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "7937e2ac22684fe3bd0e2fafbf8d0a8a",
            "1f18cae81c804bb294aa952946fee173",
            "9f5f9ff68dca43378e8499dfe9273760",
            "07e478ea22b940659f3e54039d05e85f",
            "f8b594f5e710403eaf24348e9166e9b5",
            "8f620cd50c894b0a9967c0f8bce6a241",
            "95cced1f7c834a579909c1fbc173f68c",
            "489cd7445ba14b1e97572e7cc1fac997",
            "c320c28a33a344deb432f8abb224a185",
            "b5c25afacf7b4847b689f2a41822d6d5",
            "050790d545c440189194220d297c4358",
            "4b5e87a383ab41939a7032af992d9536",
            "108b6a7a91e14bc1aca13336cfbcea0b",
            "fffde47c7a5a4d2397b9c88f6f1beeb8",
            "4de4c9192a1540b2b894438e04a8781e",
            "c8c97bcfb54d45aaa16eadac9fe34c34",
            "3337c6bd704e46debb893f58f740cda2",
            "54f5f6b0fc7a496c9254d7f9568921f7",
            "cfad47c605f34c1da8896a8b6868216d",
            "aad11568ac7f407481e5c87f17458ec9",
            "0408c99f6c3445339aeef74f54fb244e",
            "395ebc3addaf44eca64a79bd0341f5e1"
          ]
        },
        "outputId": "ef00b3e1-17fd-48b7-bf3d-507915228caa",
        "id": "5O46lSiGaAfE"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/401 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7937e2ac22684fe3bd0e2fafbf8d0a8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/50 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b5e87a383ab41939a7032af992d9536"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, EarlyStoppingCallback\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = val_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 4,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=10,\n",
        "        # max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "        metric_for_best_model = \"eval_loss\",\n",
        "        greater_is_better = False,\n",
        "        load_best_model_at_end = True\n",
        "    ),\n",
        "     callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "f52fd744-34cc-465c-d631-d444b5d43a18",
        "id": "9yfaDCzdaD_t"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 401 | Num Epochs = 4\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 200\n",
            " \"-____-\"     Number of trainable parameters = 29,884,416\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='180' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [180/200 14:12 < 01:35, 0.21 it/s, Epoch 3/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.509800</td>\n",
              "      <td>1.420282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.696500</td>\n",
              "      <td>0.702163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.639800</td>\n",
              "      <td>0.653410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.679200</td>\n",
              "      <td>0.645929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.584500</td>\n",
              "      <td>0.637426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.597200</td>\n",
              "      <td>0.634919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.716600</td>\n",
              "      <td>0.629442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.534400</td>\n",
              "      <td>0.627067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.520800</td>\n",
              "      <td>0.624361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.608000</td>\n",
              "      <td>0.622812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.590000</td>\n",
              "      <td>0.623586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.564200</td>\n",
              "      <td>0.623617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.550400</td>\n",
              "      <td>0.621582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.556200</td>\n",
              "      <td>0.621040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.587000</td>\n",
              "      <td>0.620297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.536300</td>\n",
              "      <td>0.622345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.505100</td>\n",
              "      <td>0.622092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.574500</td>\n",
              "      <td>0.622009</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
            "Using gradient accumulation will be very slightly less accurate.\n",
            "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n",
            "Could not locate the best model at outputs/checkpoint-150/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
          ]
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "tq5ehD0FaOF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"phi-3\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\", \"system\": \"system\"}, # ShareGPT style\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "index = 407\n",
        "biased_sent = test['biased_sentence'][index]\n",
        "biased_words = test['biased_words'][index]\n",
        "\n",
        "messages = [\n",
        "    {\"from\": \"system\", \"value\": \"You are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.\"},\n",
        "    {\"from\": \"human\", \"value\": \"The biased sentence is: \" + biased_sent + \"| The words identified to be carrying bias in the sentence are: \"+str(biased_words)}\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 256, use_cache = True)\n",
        "text_output = tokenizer.batch_decode(outputs)[0]\n",
        "text_output = text_output.split(\"<|assistant|>\")[1].strip()\n",
        "text_output = text_output.split(\"<|end|>\")[0].strip()\n",
        "print(text_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4938b25a-365f-4ee8-9273-584c62c1eb36",
        "id": "JAFrDDjRadeU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shelby Steele, a senior fellow at Stanford Universityâ€™s Hoover Institution said Friday that the contemporary civil rights movement under the banner of â€œBlack Lives Matterâ€ was not taking significant steps to improve the situation for black people.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test['debiased_sentence'][index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lOVws6h_rf-",
        "outputId": "5074e097-9173-4db2-fa0d-d4fde7d7bea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shelby Steele, a senior fellow at Stanford Universityâ€™s Hoover Institution, said Friday that the contemporary civil rights movement under the banner of â€œBlack Lives Matterâ€ focuses on longstanding issues but has yet to demonstrate significant progress in improving conditions for black communities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference on Test\n"
      ],
      "metadata": {
        "id": "6itwbU77aP6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_debiased_sentences(biased_sent, biased_words, tokenizer):\n",
        "    tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"phi-3\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\", \"system\": \"system\"}, # ShareGPT style\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "    messages = [\n",
        "      {\"from\": \"system\", \"value\": \"You are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.\"},\n",
        "      {\"from\": \"human\", \"value\": \"The biased sentence is: \" + biased_sent + \"| The words identified to be carrying bias in the sentence are: \"+str(biased_words)}\n",
        "    ]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize = True,\n",
        "        add_generation_prompt = True, # Must add for generation\n",
        "        return_tensors = \"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(input_ids = inputs, max_new_tokens = 256, use_cache = True)\n",
        "    text_output = tokenizer.batch_decode(outputs)[0]\n",
        "    text_output = text_output.split(\"<|assistant|>\")[1].strip()\n",
        "    text_output = text_output.split(\"<|end|>\")[0].strip()\n",
        "    return text_output\n",
        "\n",
        "to_csv = { 'gpt_debiased_sentences' : [], 'model_outputs' : [] }\n",
        "\n",
        "for biased_sent, biased_words, debiased_sent in zip(test['biased_sentence'], test['biased_words'], test['debiased_sentence']):\n",
        "    out_sentence = generate_debiased_sentences(biased_sent, biased_words, tokenizer)\n",
        "    to_csv['gpt_debiased_sentences'].append(debiased_sent)\n",
        "    to_csv['model_outputs'].append(out_sentence)\n",
        "\n",
        "df = pd.DataFrame(to_csv)\n",
        "df.to_csv('/content/drive/Shareddrives/FYP 2024-2025/Phase-2/model-evaluation/phi-3.5-mini-instruct.csv', index=False)\n"
      ],
      "metadata": {
        "id": "lWWSHXWh2Su_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phi-4"
      ],
      "metadata": {
        "id": "ZMRB1yfgam90"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Loading"
      ],
      "metadata": {
        "id": "FMVYMRl1ayyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Phi-4\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540,
          "referenced_widgets": [
            "200abed7cdd44ed78ad049dd12464b8a",
            "5f0718c2adc649d696269934a74bfd07",
            "7b7cb2b108cb4d4399dc619b9a2120c0",
            "2a1b038f6d784ce4b42a42a1c5cb86b0",
            "6077685f70e84e08958adb537727d8e1",
            "937996802232450caca9e85c3b24ea05",
            "0b1daed7ffae40c4a06f9267ff67955f",
            "fb8b7935802840a9b123885674288485",
            "eca236bdc235425fbf0ec5edf77db4f1",
            "ecec55a1533a4b94aeb4bdec486dac24",
            "66f6afc31b3e4cc4a2fee54db29a61a3",
            "ed8ac1f9cae548b4a0d4632e47908073",
            "3f225d2559574dda821ca9dadf4929ab",
            "a6633c2aba0847c38378f7b3ea25d4a6",
            "e9e74cfa6f4347e0b5f02e1033313a53",
            "7bcfb7fc02194a8b9e1a8388ebf113d8",
            "52f668ae060942a082d086cd26e6a57a",
            "ca592c173b6f42379735802b8fc4b34d",
            "c761d30d238e48b8afcefec110d70b2d",
            "5f468d6f32ea4553bcf0896a9e268656",
            "d149e2aac48e441db1d8f3aa26e0e7f5",
            "92c860a0d7724951a1c0da311fc96ec2",
            "2a4a546fdf0144ac83de06d0583d3fc3",
            "2f2b8a83e3964391b73c4cd063ea2cc4",
            "a3e7843264be42b89bb8f4fc86f76519",
            "8d26fd4aff274482a1d597e95432a047",
            "2f342340fa1d49f383552226f5999042",
            "3ea31a1a9eb748e0843cd866eafa78c6",
            "e9c86220d77d466d889dcce11fd43144",
            "2ccac48ca709439eb4957841a683cd3a",
            "987af2f3b2364602ada4fd519c8057c0",
            "3d736527b1f242639736340f183f02b5",
            "aa081c5838ba4f1385777cca45629d73",
            "2507f91512d34aa7a567cc63f42b55a8",
            "716f3925b2db43c096f3b0f21c90f060",
            "2d0880ad966b43deaf9c29c2201abab1",
            "a62c6b27da9b4e14beda6f11c4c27378",
            "82e5f3948d544581ae343ac4e3c7f039",
            "fc0e626a608747ef81b3f3ed8aa45c63",
            "9f55b5e8d9214a6bbd469fdf2ec1779f",
            "d395907ef8b044d3844b850bf2bcafe8",
            "3143a9810e894bfb816a15b823c44913",
            "c834c5395e354f44b62f19375fca17d4",
            "a8eaadf5ad8f4d83a71fa2147783c486",
            "0cd670c88b904918981aab659e95ed07",
            "eba18124eac348e9a879589f0469e0a5",
            "18dbf72ba0254155ae40d1ce864e01fa",
            "e0018d3e90f24b4ea46318635609ce2a",
            "f7d6a113f4094b09ba5f39ebd892d26e",
            "abea850d861c43ea863913e199d44cf7",
            "c6ca31a6fa234d16ba4cb3aa67495ec1",
            "9ad8774a8c164cc3873bcc7f3353309e",
            "6527afcf15c746b1ad40d3c4181138e1",
            "0e04c14b1b5e4935bf960b367b5d91ea",
            "d3d4251404284e0db9f78c8f469aa65f",
            "0e01c47d6ca34e6ca73ec8b9ab91370e",
            "6678d90081c140039a19fead00447bad",
            "4e6ae7d45b184def91fe4f97a9cecdaa",
            "f66e65c00327477393c33fd031c4f8d7",
            "c9504b0e201c4b80b72227aba3871498",
            "62c9c13f5ae14f51b5447e0d92158401",
            "ac50a67e24fc43639abbd1bf3582953e",
            "703a2f47374b4975971992774002345a",
            "1a12238a37ae4e6f91e88cab5d9544e0",
            "11ead1067e07476cb353e9d0adca7579",
            "c25e536127654ea7b2bba8ceedd1ff10",
            "43b926e2dc3545b0b4d276b88e4e9d76",
            "1b4c97f1014d40f8aefe91b101c9d408",
            "c3b1ab7e1af04f56aa9e09412c960b54",
            "c25bba8dcf144872ac15f68be5ec4d5c",
            "badb2fd4bc1e44039c5eef80693c3009",
            "2c5f62549a41497b99bd22130710a464",
            "65aedd91aba34bc497e1fb58a73a9e5b",
            "621ed36987cc4372a78f5beee6596ca6",
            "dc4e3b9dba34446fb32ea534a0ae4422",
            "41bdd195a15b4f9c84a0d7d2d2d779f9",
            "fefd5d15279545a9923cbfe2b90a0ef0",
            "b968d5c0c9604eb3b934dcfbebdfb6c8",
            "26cca2e7a3234449a99612fcc2cead0a",
            "4ad3299d183d40ba9173f7e4d64ef600",
            "5d64ebdd07aa494c872b95999053488a",
            "85cc9c6e4e2c4bffaa9ce2835c8f0a49",
            "12ce667ba8a246a19eb996e2aebacb52",
            "5ea1cbbc96114e21a8da4b60f0203b0c",
            "0e768cc3c87a4b2d99184f02edcc55c6",
            "26349606d94d41ed83c4b823392c0f70",
            "685f28b83fd44da392abfe093672773b",
            "2b8e09aebf3c430bbc75c5352e0a7bd7",
            "51209b3ec2444cbc975a289a5dbea3b6",
            "8b25b4664b8b4752a8ffe03f4f07228d",
            "d37a8ad56a564d2a8d5ba5ede7405469",
            "2e55f0a35b9247f384cd5592a5f64f38",
            "d3c4db89e8db41ea8c381b47b27e9e31",
            "6bdc6aa6ed8c4b4f8c7a38bea849d6f5",
            "04acc575d7714a0793fae4be20a2138b",
            "7ee9c304062a45db9e58e85bb77cdedc",
            "7ffe63bca0be4b5c82fc8ddd46ba9586",
            "6d038bd8adae421189cb02f4545f0b49",
            "33d7b09de0da4d5f9bc9e8525b06b703",
            "b4396d8800b04dd799e1e5f9f2595acf",
            "da738dceac664bb593e99e740b4d821a",
            "6cd8a41414fd42e098f374036c19a14e",
            "d55ed5bae1c94325baeb14d856be9519",
            "428ad0b55d304d57a809a5f00bfe26d9",
            "7f3183383e0b434e823df50c133ce9a5",
            "19c5e63e3bcf4b798338fbe6d30b368e",
            "f8454cca1dbb45bcac86c0951252209e",
            "8edee75080244ac6805321105256ac91",
            "57e0ca086a984320ad8766c9fcabd925",
            "0e5e675e41514bf1a280fb7f3131e198",
            "94433d10d29243cabbfdee59aff1eed2",
            "e6b67b3c92684e9c8c9ed08faaf542fb",
            "1340539dbe5545b682555de89d74625b",
            "6444e68f146c4d3c83384672e3f5a5c6",
            "925037cc355e47ce9554f2561ffc46a4",
            "9e4d9cef81924fb88430ef8ff4a42f0c",
            "f8ff041454424c8482c9fd859f75dde2",
            "f4e0f2a246c74775bdab54145562abc2",
            "b2d25bbf82c14f278f191c3e0600cd93",
            "af60ecf3c9bf4bd2adfab55e99b58030",
            "782769c0efc44243a2b60af85e4f1ca8",
            "8cc7da3aa7af4fab925dc3741f52f4c3",
            "f2be92bb4cad4adfad43da1dca302ec6",
            "5ce50f476dd547b3abbb427646209a9f",
            "1f53743bf937491cb8d539ea566c309e",
            "f291a8fc17d54a9ba8ecbb76e57a1d18",
            "06b45bbf3a8a4378a7f6747f73b7fcae",
            "b49ad83f52884db5a5fd9fc80138b885",
            "73ca3fc4e14c43a9bde4eed8d1fdaf87",
            "334f6acb6cf040c8b42e66f39f422d11",
            "efd31041de6e4179b82fb00e002500ac",
            "e2e38c0f97e9433182be994d3f11fdda"
          ]
        },
        "id": "23D49F6w3S4M",
        "outputId": "74877a41-3d66-44d8-f723-3e2eaa4f94e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.1.7: Fast Llama patching. Transformers: 4.47.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/160k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "200abed7cdd44ed78ad049dd12464b8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed8ac1f9cae548b4a0d4632e47908073"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a4a546fdf0144ac83de06d0583d3fc3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/4.39G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2507f91512d34aa7a567cc63f42b55a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/1.03G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0cd670c88b904918981aab659e95ed07"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e01c47d6ca34e6ca73ec8b9ab91370e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/170 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43b926e2dc3545b0b4d276b88e4e9d76"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/18.0k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b968d5c0c9604eb3b934dcfbebdfb6c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.61M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51209b3ec2444cbc975a289a5dbea3b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/917k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4396d8800b04dd799e1e5f9f2595acf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94433d10d29243cabbfdee59aff1eed2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.15M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8cc7da3aa7af4fab925dc3741f52f4c3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-Shot"
      ],
      "metadata": {
        "id": "pd50NAMta4bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "def get_zero_shot(biased_sent, biased_words, tokenizer):\n",
        "  messages = [\n",
        "      {'content': \"You are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content and structure is retained.\",\n",
        "    'role': 'system'},\n",
        "      {'content': \"The biased sentence is: \" + biased_sent + \"| The words identified to be carrying bias in the sentence are: \"+str(biased_words),\n",
        "        'role': 'user'}\n",
        "  ]\n",
        "  inputs = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize = True,\n",
        "      add_generation_prompt = True, # Must add for generation\n",
        "      return_tensors = \"pt\",\n",
        "  ).to(\"cuda\")\n",
        "\n",
        "  output = model.generate(\n",
        "      input_ids = inputs, max_new_tokens = 128, use_cache = True, temperature = 1, min_p = 0.1\n",
        "  )\n",
        "  output = tokenizer.batch_decode(output)\n",
        "  output = output[0].split('<|im_start|>assistant<|im_sep|>')[-1]\n",
        "  output = output.split('<|im_end|>')[0]\n",
        "  return output\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "      tokenizer,\n",
        "      chat_template = \"phi-4\",\n",
        "  )\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "to_csv = {'biased_sentence': [], 'debiased_sentence': [], 'model_output': []}\n",
        "condition = True\n",
        "for biased_sent, biased_words, debiased_sentence in zip(test['biased_sentence'], test['biased_words'], test['debiased_sentence']):\n",
        "  model_output = get_zero_shot(biased_sent, biased_words, tokenizer)\n",
        "  if condition:\n",
        "    print(model_output)\n",
        "    condition = False\n",
        "  to_csv['biased_sentence'].append(biased_sent)\n",
        "  to_csv['debiased_sentence'].append(debiased_sentence)\n",
        "  to_csv['model_output'].append(model_output)\n",
        "\n",
        "df = pd.DataFrame(to_csv)\n",
        "df.to_csv('/content/drive/Shareddrives/FYP 2024-2025/Phase-2/model-evaluation/phi_4_zero_shot_results.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jE9hAnY23eq0",
        "outputId": "712230db-5359-4b75-86ff-df3949744ed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The group opposing vaccination subscribes to a theory not supported by scientific evidence, suggesting that childhood vaccinations are harmful and may lead to various health issues, including autism. They disseminate these views through YouTube, Facebook, and other online platforms.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few-Shot"
      ],
      "metadata": {
        "id": "avM-vOjAbAlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "def get_few_shot(biased_sent, biased_words, tokenizer):\n",
        "    # Few-shot examples\n",
        "    few_shot_examples = [\n",
        "        {\n",
        "            'biased': \"YouTube is making clear there will be no â€œbirtherismâ€ on its platform during this yearâ€™s U.S. presidential election â€“ a belated response to a type of conspiracy theory more prevalent in the 2012 race.\",\n",
        "            'biased_words':  ['belated', 'birtherism'],\n",
        "            'unbiased': \"YouTube is making clear there will be no conspiracy theories related to candidatesâ€™ birthplaces on its platform during this yearâ€™s U.S. presidential election â€“ a response to similar theories prevalent in the 2012 race.\"\n",
        "        },\n",
        "        {\n",
        "            'biased': \"Looking around the United States, there is never enough welfare for the left to stop killing developing humans in uteroâ€”solidly Democratic states lead the nation in abortion rates.\",\n",
        "            'biased_words':  ['killing', 'never', 'developing', 'humans', 'enough'],\n",
        "            'unbiased': \"Looking around the United States, there is ongoing debate about welfare and reproductive rightsâ€”solidly Democratic states have higher abortion rates.\"\n",
        "        },\n",
        "        {\n",
        "            'biased': \"Even Richard Nixon, to whom Trump likes to compares himself to, was occasionally willing to venture out into crowds of protesters to talk to them. Trump, however, is such a wuss that he made his cadre of protectors tear-gas people rather than subject himself to a crowdâ€™s insults. Far from proving he was a tough guy, Trumpâ€™s actions only led to another round of taunting about his wimpiness.\",\n",
        "            'biased_words': ['wuss', 'wimpiness'],\n",
        "            'unbiased': \"Even Richard Nixon, to whom Trump likes to compare himself, was occasionally willing to venture out into crowds of protesters to talk to them. Trump, however, chose to have his cadre of protectors use tear gas rather than engage with the crowd. Far from proving he was a tough guy, Trumpâ€™s actions led to further criticism about his reluctance to confront the crowd directly.\"\n",
        "        },\n",
        "        {\n",
        "            'biased': \"The explosion of the Hispanic population has long-term job prospect consequences as well: Both legal and illegal aliens will occupy 75 percent of new American jobs in as little as five years.\",\n",
        "            'biased_words': ['explosion'],\n",
        "            'unbiased': \"The rapid growth of the Hispanic population has long-term job prospect implications as well: Both legal and undocumented immigrants are projected to occupy 75 percent of new American jobs in as little as five years.\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Constructing the prompt with few-shot examples\n",
        "    messages = [\n",
        "        {'content': \"You are provided with a linguistically biased sentence from a news article along with words identified to be carrying the bias. Linguistic bias refers to the use of words, phrases, or structures that carry a subjective tone, either overly positive or overly negative. Such biases can influence the reader's perception, making the text less neutral. Your task is to rephrase the given sentence to remove or neutralize the bias in such a way that most of its original meaning, content, and structure is retained.\",\n",
        "         'role': 'system'}\n",
        "    ]\n",
        "\n",
        "    for example in few_shot_examples:\n",
        "        messages.append({\n",
        "            'content': f\"The biased sentence is: {example['biased']} | The words identified to be carrying bias in the sentence are: {example['biased_words']} | The unbiased rephrased sentence is: {example['unbiased']}\",\n",
        "            'role': 'user'\n",
        "        })\n",
        "\n",
        "    # Adding the test sentence\n",
        "    messages.append({\n",
        "        'content': f\"The biased sentence is: {biased_sent} | The words identified to be carrying bias in the sentence are: {biased_words}\",\n",
        "        'role': 'user'\n",
        "    })\n",
        "\n",
        "    # Tokenizing the input\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,  # Must add for generation\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Generating the output\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs, max_new_tokens=128, use_cache=True, temperature=1, min_p=0.1\n",
        "    )\n",
        "    output = tokenizer.batch_decode(outputs)\n",
        "    # print(output)\n",
        "    output = output[0].split('<|im_start|>assistant<|im_sep|>')[-1]\n",
        "    output = output.split('<|im_end|>')[0]\n",
        "    return output\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "      tokenizer,\n",
        "      chat_template = \"phi-4\",\n",
        "  )\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "to_csv = {'biased_sentence': [], 'debiased_sentence': [], 'model_output': []}\n",
        "condition = True\n",
        "for biased_sent, biased_words, debiased_sentence in zip(test['biased_sentence'], test['biased_words'], test['debiased_sentence']):\n",
        "  model_output = get_few_shot(biased_sent, biased_words, tokenizer)\n",
        "  if condition:\n",
        "    print(model_output)\n",
        "    condition = False\n",
        "  to_csv['biased_sentence'].append(biased_sent)\n",
        "  to_csv['debiased_sentence'].append(debiased_sentence)\n",
        "  to_csv['model_output'].append(model_output)\n",
        "\n",
        "df = pd.DataFrame(to_csv)\n",
        "df.to_csv('/content/drive/Shareddrives/FYP 2024-2025/Phase-2/model-evaluation/phi_4_few-4_shot_results.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_8jJrC2bCvY",
        "outputId": "137f01e5-959b-4a52-929e-243747ec19a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The unbiased rephrased sentence is: The group opposing vaccinations believes that childhood vaccinations can be harmful and may cause various health issues, including autism, and shares these beliefs through YouTube, Facebook, and other online platforms.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "vN-F34DqbcDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phi-3.5-mini"
      ],
      "metadata": {
        "id": "TvuGutn4RMYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phi_output = pd.read_csv('/content/drive/Shareddrives/FYP 2024-2025/Phase-2/model-evaluation/phi-3.5-mini-instruct.csv')\n",
        "references = phi_output[\"gpt_debiased_sentences\"].tolist()\n",
        "predictions = phi_output[\"model_outputs\"].tolist()"
      ],
      "metadata": {
        "id": "AmezzU4vPeSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meteor = evaluate.load('meteor')\n",
        "results = meteor.compute(predictions=predictions, references=references)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "40e4f8931a7c49988fba71bb9cb310d7",
            "e156260098814f569bde795b391b815d",
            "67e7450ac54248309d94f8ad641dd5a6",
            "51b443a672d74f71a1df3ade9b9ee3ce",
            "d0a1220ab1664c26aff5a4c4b9753d34",
            "d99e15d110194240b5cb16aa8141f13f",
            "7f20a15c5f6a42b49a33da7d13ee551c",
            "4bb1a1175c894d1ba09009580aed366c",
            "10c88668b96443c78edcf1e0f15e7366",
            "0ea181423bb348369c971bdcac7b7872",
            "91ed64105d2845bf9ec581f1257655b3"
          ]
        },
        "id": "Jh-n-dpwR5-Z",
        "outputId": "24456df9-6bec-4085-cc95-8a2c1a7535f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/7.02k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40e4f8931a7c49988fba71bb9cb310d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'meteor': 0.8571112509612245}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = evaluate.load('bleu')\n",
        "results = bleu.compute(predictions=predictions, references=references)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "39e09e8ac4c64f999294feeb5761a9ba",
            "6b89f5b08fc04603b61d654f1d2fc454",
            "4571c6efb5fe42aaba62cd6a39b134e4",
            "3c3859b9457b4a3992ae2d2d48be6de8",
            "196df750f8cf4cba8cf720d772af6923",
            "d344d223f5a644658d9ec8e4ba3ea989",
            "303ec6fd557a4e5e9d3e079014137a97",
            "e6e8616bdb144632ab2f2c7cf03a18d1",
            "5e3cce8e941643be92cf88a96a788750",
            "b3ffcb9bdd5b4eb48054882d01163790",
            "1694833ec198402aa365dbf120de69b3",
            "fece70c71ff64b58a9dab64fd44cf634",
            "ffbf43a77c9348cd85f5b78953d6e2c7",
            "636af3d2e75b4f9abce084fdd5322ea1",
            "8b166e7b9be44636a3bccee1d3bf22d8",
            "ec3e408bd39f42c196c0ea8821e82deb",
            "405f9e2faf9742a29becb1ea83d11501",
            "079723fa320543d9ae9d487451ee58c2",
            "6015dc7ea75b448aa362cee8d6e7eda7",
            "9c8c8fd5782c456ea1a2eff2beb2ed0b",
            "7b70d4d74b214661b92f9bcc09b6e08b",
            "68f81acba5214a8ca6e1d8b4cb63683a",
            "03882f8c2ddb4a1abd16588592864cd5",
            "eb64375153ef426792f6240c5e1e6a09",
            "5c5edacb9cf54d9ab1792cf13fa38eeb",
            "82182ddb57594f6bba9cdc8eeceef741",
            "a076cbe8fb4348c7af64034805cdfe22",
            "f601eee2846446b9b33f7e46cdc5ba34",
            "cfe06d609eb94e24aaa3e0960540944d",
            "1802f35e9ac74e8eb59789ec967cb02d",
            "0865d29ee4944c3e9841f89adf29c59b",
            "51baae5ab0634707aa56e1e52abdad95",
            "16084f8c44ea44f1b6685834de97ae65"
          ]
        },
        "id": "VAlP4M60SRR7",
        "outputId": "0ba022e4-7689-4bc7-f1a7-f43ccda42ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39e09e8ac4c64f999294feeb5761a9ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fece70c71ff64b58a9dab64fd44cf634"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03882f8c2ddb4a1abd16588592864cd5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bleu': 0.7036358443890706,\n",
              " 'precisions': [0.8269951794322442,\n",
              "  0.7274229074889867,\n",
              "  0.6651558073654391,\n",
              "  0.6126021003500584],\n",
              " 'brevity_penalty': 1.0,\n",
              " 'length_ratio': 1.0418526785714286,\n",
              " 'translation_length': 1867,\n",
              " 'reference_length': 1792}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "results = rouge.compute(predictions=predictions, references=references)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "05fd14f3c5e94fb3a1bbea07f38a338d",
            "b2291dcc525b49fca1f0892d417e4670",
            "64e312aa3a5c43c08b22aaae3a32190a",
            "b1f067ca7a8d469bbd7ac914892d8dad",
            "dd783f2defcc4c3180661091a5bd0936",
            "4475b6fe61364466be9b1108033c529d",
            "83b95a71958f4f3b897bb58756ca3d9b",
            "bac6376250e24ce7a4b882725b6f92a8",
            "4c4566eacf5b4a2594fd1bded05793bf",
            "e536f7148aea4652b1a3a1ced807ff69",
            "7510268ae56346cf96f9d489cd46906a"
          ]
        },
        "id": "3SlQi4u4SYx9",
        "outputId": "ce918854-31b5-498d-bac8-5aad9dd67741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05fd14f3c5e94fb3a1bbea07f38a338d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': 0.834295713268077,\n",
              " 'rouge2': 0.7346169759486485,\n",
              " 'rougeL': 0.8191233132846822,\n",
              " 'rougeLsum': 0.8192020400894033}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bertscore = evaluate.load('bertscore')\n",
        "results = bertscore.compute(predictions=predictions, references=references, lang='en')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3328e194f9274b50b8cb717d4ec54e59",
            "9f527ae4494949d08bea286ad9d90f2a",
            "ecf2398105604d6f8585409446873c61",
            "fd59114499674ba48fa6b1ef72bb9bf9",
            "56a42a5edc594fc6a1adfdc414e88176",
            "bb7e03d1ff1b42aa9883c4c4fafe5c94",
            "91849229242b4d1887a9b728995e2c48",
            "7bebdf3d6d344481987c1997c127d27f",
            "1098e28a678c40a1b7dbfea84a1833fd",
            "7677511b98b84e36b1200c14922d1e44",
            "0a49e52fce894147a19b7d7214e2821c",
            "7050643cacec4bce9120a8e45becc82b",
            "2fa6dc82be6642fcab76a6d6a2251e31",
            "31619499585a4c4799f683cc5ca1bb09",
            "b24cf524a2574b92b3eac7d66ece688d",
            "22adb80a6ecd4d778c19aebb174546a6",
            "520bceb5165f4dab8c0f1f17686478c0",
            "c33defbe841b4714a6db66c9f4fd8bec",
            "a9212130ac034ad4af405449c64ac298",
            "741ff27e76e6440cad28d21068c0fe42",
            "226ba9bd664b4502936193dd7293d8ab",
            "b353acd35ab8494c801b415564df5c1a",
            "e55712d68a684ccab33ee1210516f16f",
            "32987c31a1584fb6a35a8f0e84d74c20",
            "d33fa7d7db8f47bcaf08004b7e5dcf72",
            "be1e1e23e9c84113970e471ffb99feb7",
            "c061e88192d24a2dbac5f24b595a1dc4",
            "105fd5c7f78b41038d8535dd1fceabec",
            "b336ce030fee4f8cb0fa34a43dfafed1",
            "290ef8ae174b4c4786e4f8146a1a8c79",
            "deab6c97e6064b9fa0cb1eec2c150e68",
            "2c2d0e539e054ae7b81ca1cbb60c3f72",
            "a5ab9d542a9047639a7f3996572fab24",
            "375631251d6b4234bc97e2a66539a5aa",
            "1b916d4510dd49ec8b4e2c90dd4e2711",
            "b51d3ad97af346239da6ca26a98c707a",
            "f208e5801e614038adfc957c759cc247",
            "d9f30ebe25fd4aafa3cfb458600a2ddb",
            "481ef42e94fd44beb40e25495e8d7452",
            "72d79f49860d40a9a3939ef82dab9fe4",
            "1e7eefbd26ea4fba861eea3c8d5e4460",
            "f5fd69ff4bce46d98a15f4e50aabe105",
            "2dcd6c488eff475ea6a640be229024cb",
            "cf96d7a9c23842dfa669d570106c66c2",
            "88f1d40e93b642cab72191161ba64f42",
            "eac2e23e729346559b56600289075dae",
            "cb40418d253b48048d21335d5775dfec",
            "0544cd5058c2495498228fcb02414d5d",
            "a9ea6981b73a47f2be04607ffe606179",
            "ccf8110722974b819c82a4490f1f43e5",
            "49e4315e85b9453598ce80e07a1f530d",
            "5b6c22bc65194898af10d2ee4687f879",
            "08efdf0b30ca4b7881d06ba336372e91",
            "c63465dce09541e7aafdfd4dd9af8e37",
            "b87ae0708abb453bb2822af8843ec31e",
            "55450e396df4428193c1b8807c539852",
            "fdf66bf83a514d1db401ebfb9f38bcd9",
            "07ad153cd5f14036b605caaa7e5e6226",
            "52bdc46559734e7da3a65e9e8dba8f16",
            "366b8de39b09474e8847ecb7b7a65288",
            "440e95d2e07646e5961331e8b7f8a870",
            "5995aa3482274f03b74f778f2a6c7152",
            "366e6988b05f42f99b2d11cc16474423",
            "5ef888cd94ad470c99e7ecce4bab4aa7",
            "7586af2ef9ce4b5aaad0c0b6b968d84a",
            "ea69d43967824fc2b0f6efb6457b55c1"
          ]
        },
        "id": "NTpYLHUvSfT6",
        "outputId": "bc140692-1abf-4af9-ddd1-46a5ee641496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3328e194f9274b50b8cb717d4ec54e59"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7050643cacec4bce9120a8e45becc82b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e55712d68a684ccab33ee1210516f16f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "375631251d6b4234bc97e2a66539a5aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88f1d40e93b642cab72191161ba64f42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55450e396df4428193c1b8807c539852"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'precision': [0.9515666961669922,\n",
              "  0.9974159598350525,\n",
              "  0.9465359449386597,\n",
              "  0.9745163321495056,\n",
              "  0.9862303733825684,\n",
              "  0.9948627948760986,\n",
              "  0.9935314059257507,\n",
              "  0.988091766834259,\n",
              "  0.9669088125228882,\n",
              "  1.0,\n",
              "  1.0000001192092896,\n",
              "  0.9953920245170593,\n",
              "  0.9796689748764038,\n",
              "  0.9901639223098755,\n",
              "  0.9687455296516418,\n",
              "  0.9502426385879517,\n",
              "  0.8785829544067383,\n",
              "  0.9892180562019348,\n",
              "  0.976586103439331,\n",
              "  0.9956639409065247,\n",
              "  0.9953327178955078,\n",
              "  0.9909133911132812,\n",
              "  0.9326778650283813,\n",
              "  0.9030010104179382,\n",
              "  0.9799964427947998,\n",
              "  0.9848775863647461,\n",
              "  0.9847591519355774,\n",
              "  0.9861161708831787,\n",
              "  1.0,\n",
              "  0.9596093893051147,\n",
              "  1.0,\n",
              "  0.9428414106369019,\n",
              "  0.958696186542511,\n",
              "  0.9127467274665833,\n",
              "  0.9823418259620667,\n",
              "  1.0,\n",
              "  0.9811862707138062,\n",
              "  1.0,\n",
              "  0.9690479636192322,\n",
              "  0.9535700678825378,\n",
              "  0.9814461469650269,\n",
              "  0.9884414672851562,\n",
              "  0.9914618134498596,\n",
              "  0.9825743436813354,\n",
              "  0.9999998807907104,\n",
              "  0.9521896243095398,\n",
              "  0.9789704084396362,\n",
              "  0.9826786518096924,\n",
              "  0.9359189867973328,\n",
              "  0.9916688203811646,\n",
              "  0.9707595109939575],\n",
              " 'recall': [0.9558926820755005,\n",
              "  0.9974159598350525,\n",
              "  0.9435135722160339,\n",
              "  0.941699206829071,\n",
              "  0.973459780216217,\n",
              "  0.992644190788269,\n",
              "  0.99322110414505,\n",
              "  0.9824806451797485,\n",
              "  0.9732937216758728,\n",
              "  1.0,\n",
              "  1.0000001192092896,\n",
              "  0.9912625551223755,\n",
              "  0.9807394742965698,\n",
              "  0.9757591485977173,\n",
              "  0.9685777425765991,\n",
              "  0.9635841846466064,\n",
              "  0.9525988698005676,\n",
              "  0.994238018989563,\n",
              "  0.9869512319564819,\n",
              "  0.9956639409065247,\n",
              "  0.9953327178955078,\n",
              "  0.9915907382965088,\n",
              "  0.9612871408462524,\n",
              "  0.9477699995040894,\n",
              "  0.9756596088409424,\n",
              "  0.9793175458908081,\n",
              "  0.9816024899482727,\n",
              "  0.9903313517570496,\n",
              "  1.0,\n",
              "  0.9580850601196289,\n",
              "  1.0,\n",
              "  0.9402146339416504,\n",
              "  0.9749903678894043,\n",
              "  0.940953254699707,\n",
              "  0.9888734817504883,\n",
              "  1.0,\n",
              "  0.9763474464416504,\n",
              "  1.0,\n",
              "  0.9688782691955566,\n",
              "  0.958675742149353,\n",
              "  0.9904615879058838,\n",
              "  0.9898185729980469,\n",
              "  0.9846262335777283,\n",
              "  0.9844654202461243,\n",
              "  0.9999998807907104,\n",
              "  0.9611653685569763,\n",
              "  0.9639138579368591,\n",
              "  0.972782552242279,\n",
              "  0.9573882222175598,\n",
              "  0.9909122586250305,\n",
              "  0.9720202088356018],\n",
              " 'f1': [0.9537248015403748,\n",
              "  0.9974159598350525,\n",
              "  0.9450223445892334,\n",
              "  0.9578267335891724,\n",
              "  0.9798035025596619,\n",
              "  0.9937522411346436,\n",
              "  0.9933762550354004,\n",
              "  0.9852781891822815,\n",
              "  0.9700908064842224,\n",
              "  1.0,\n",
              "  1.0000001192092896,\n",
              "  0.993323028087616,\n",
              "  0.9802039265632629,\n",
              "  0.9829087257385254,\n",
              "  0.9686616063117981,\n",
              "  0.9568669199943542,\n",
              "  0.9140951037406921,\n",
              "  0.9917216300964355,\n",
              "  0.9817413091659546,\n",
              "  0.9956639409065247,\n",
              "  0.9953327178955078,\n",
              "  0.9912519454956055,\n",
              "  0.9467663764953613,\n",
              "  0.9248440861701965,\n",
              "  0.9778232574462891,\n",
              "  0.9820896983146667,\n",
              "  0.9831782579421997,\n",
              "  0.9882192015647888,\n",
              "  1.0,\n",
              "  0.9588465690612793,\n",
              "  1.0,\n",
              "  0.9415261745452881,\n",
              "  0.9667746424674988,\n",
              "  0.9266354441642761,\n",
              "  0.9855968356132507,\n",
              "  1.0,\n",
              "  0.9787608981132507,\n",
              "  1.0,\n",
              "  0.968963086605072,\n",
              "  0.9561160802841187,\n",
              "  0.985933244228363,\n",
              "  0.9891295433044434,\n",
              "  0.9880321621894836,\n",
              "  0.9835189580917358,\n",
              "  0.9999998807907104,\n",
              "  0.9566564559936523,\n",
              "  0.971383810043335,\n",
              "  0.9777054786682129,\n",
              "  0.9465318918228149,\n",
              "  0.9912903308868408,\n",
              "  0.9713894128799438],\n",
              " 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.47.1)'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, _ in results.items() :\n",
        "    print(key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkYsKFeETltc",
        "outputId": "2ee59241-49ca-4f30-a067-7f712f40c5e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision\n",
            "recall\n",
            "f1\n",
            "hashcode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(results['precision'])/len(results['precision'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo5i02JNTYdH",
        "outputId": "03d0d4ad-8558-40ba-d041-d4296d93cf25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9744656506706687"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(results['recall'])/len(results['recall'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzDWEAODTg8g",
        "outputId": "31211635-b28d-4421-8d78-68ca1325e183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9776560816110349"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(results['f1'])/len(results['f1'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7JIdWi1SlgB",
        "outputId": "db7ae944-083d-4bf3-afea-cf2c5631df70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9759955604871114"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Llama 3.2"
      ],
      "metadata": {
        "id": "iArG45mXT3MK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llama_output = pd.read_csv('/content/drive/Shareddrives/FYP 2024-2025/Phase-2/model-evaluation/llama-3.2-3b.csv')\n",
        "references = llama_output[\"gpt_debiased_sentences\"].tolist()\n",
        "predictions = llama_output[\"model_outputs\"].tolist()"
      ],
      "metadata": {
        "id": "hNv7RN3KTCKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = meteor.compute(predictions=predictions, references=references)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEf_mU0eUKBO",
        "outputId": "1b861526-0571-4342-fb33-eb17075f7e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'meteor': 0.7794488656934015}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = bleu.compute(predictions=predictions, references=references)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M9k5Mu4UO2x",
        "outputId": "1cacff84-9596-42c9-821f-4b1d8e43ab4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bleu': 0.6168166231415156,\n",
              " 'precisions': [0.7634408602150538,\n",
              "  0.6434494195688225,\n",
              "  0.5722411831626849,\n",
              "  0.5149384885764499],\n",
              " 'brevity_penalty': 1.0,\n",
              " 'length_ratio': 1.0379464285714286,\n",
              " 'translation_length': 1860,\n",
              " 'reference_length': 1792}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = rouge.compute(predictions=predictions, references=references)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsmR2_miUSnO",
        "outputId": "eb48a2d7-73d2-4ba8-e9bd-e20b2f84f69f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': 0.7563697413677066,\n",
              " 'rouge2': 0.6362580011289978,\n",
              " 'rougeL': 0.7396826752236316,\n",
              " 'rougeLsum': 0.7404612287333479}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = bertscore.compute(predictions=predictions, references=references, lang='en')"
      ],
      "metadata": {
        "id": "CK13R_wXUWEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(results['precision'])/len(results['precision'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBZXUFkbUa1Y",
        "outputId": "a115d5f1-729d-4252-f577-ee8fc05f38ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9655002823062971"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(results['recall'])/len(results['recall'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzFG490vUe9x",
        "outputId": "2552110b-d651-4581-bbbf-72c15da820b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9681099524684981"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(results['f1'])/len(results['f1'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hQlmIkDUhUC",
        "outputId": "0c02ed12-e289-467d-f27d-f1ebdf84679b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9667413363269731"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mistral v0.3"
      ],
      "metadata": {
        "id": "V-ozmcuzUlyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mistral_output = pd.read_csv('/content/drive/Shareddrives/FYP 2024-2025/Phase-2/model-evaluation/mistral_7B.csv')\n",
        "# mistral_output.columns\n",
        "references = mistral_output[\"debiased_sentence\"].tolist()\n",
        "predictions = mistral_output[\"model_output\"].tolist()"
      ],
      "metadata": {
        "id": "p-DWBA3qUpPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = meteor.compute(predictions=predictions, references=references)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2YOZgyyU32D",
        "outputId": "46bba1e6-88c6-4fd7-96c8-4b497301ebd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'meteor': 0.8517611694189426}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = bleu.compute(predictions=predictions, references=references)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvsPcrfEU_m_",
        "outputId": "729dcc81-a708-465f-cd00-07ef88d38154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bleu': 0.689223691034105,\n",
              " 'precisions': [0.8133895624670533,\n",
              "  0.7134344528710725,\n",
              "  0.6495821727019498,\n",
              "  0.5986238532110092],\n",
              " 'brevity_penalty': 1.0,\n",
              " 'length_ratio': 1.05859375,\n",
              " 'translation_length': 1897,\n",
              " 'reference_length': 1792}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = rouge.compute(predictions=predictions, references=references)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ll9Df8zPVDOD",
        "outputId": "296f3e98-8ca7-42c9-9f40-e89018bed339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': 0.8240452956105762,\n",
              " 'rouge2': 0.7229973757211998,\n",
              " 'rougeL': 0.8109019257978958,\n",
              " 'rougeLsum': 0.8109204661881106}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = bertscore.compute(predictions=predictions, references=references, lang='en')"
      ],
      "metadata": {
        "id": "fVP8lTyuVGMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(results['precision'])/len(results['precision'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciZT-mSKVJB0",
        "outputId": "9150d021-aef2-49e8-9451-f2c64e24d16c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9720884187548768"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(results['recall'])/len(results['recall'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI9B42UTVLup",
        "outputId": "36e0ecb8-7743-4b93-82da-3d11baca5f0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.977469730610941"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(results['f1'])/len(results['f1'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KbMBUBzVNyi",
        "outputId": "61fb3c56-4dba-49e0-8240-7bfa0ef8fd43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9747044105155795"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phi-4 Zero Shot"
      ],
      "metadata": {
        "id": "islBvCh1iItN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot = pd.read_csv('/content/drive/Shareddrives/FYP 2024-2025/Phase-2/model-evaluation/phi-4-zero-processed.csv')\n",
        "# zero_shot.columns\n",
        "references = zero_shot[\"debiased_sentence\"].tolist()\n",
        "predictions = zero_shot[\"model_output\"].tolist()"
      ],
      "metadata": {
        "id": "DnDDvscVVXub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meteor = evaluate.load('meteor')\n",
        "results = meteor.compute(predictions=predictions, references=references)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "5d274715c1d547f58e50bad57cea3d8a",
            "eb321a9b3d6b45669b4779ecf182e26b",
            "04564d9847b1410eb816a92cf1e4378b",
            "33f3ee73277c4e21adab9a088fdc1c5c",
            "b0da66c5d1e84c00b034246c6a4daa4e",
            "95d11b3efec64e57851c180f47ab1d17",
            "73a84b1073f445ea891ecf540c6cf5db",
            "1967b9f893ec4966acd08668be34b678",
            "9499529f59b14afe8a4c0129d0091888",
            "3036063d8e374a22b123d4aa8dab72b1",
            "ed3293523af342aaaf79ce415b85bbea"
          ]
        },
        "id": "ZIFki9q1ilTM",
        "outputId": "ac98523c-796a-41d1-c159-400fa90eea42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/7.02k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d274715c1d547f58e50bad57cea3d8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'meteor': 0.7230837260333871}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = evaluate.load('bleu')\n",
        "results = bleu.compute(predictions=predictions, references=references)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "2660198e4ba44d97ad27150b3f8f2f33",
            "0046e4b0876d47a487de579b28a400a5",
            "92678143eac74369b13742da22d3497a",
            "8907e4d719a747b1b6933fa64df528b3",
            "eaa7bea410464707b853ee1fe12fb5b1",
            "c7d2e9afecc54e93bcc66659eda5b658",
            "6f805c00ab074921adec0268d122cf8a",
            "44ae4290c8fd4b50b8d8f6b391ede830",
            "76ba7209147a4df1b057c2902bbd07e5",
            "185e0494d6604e9e997d9e332315dd70",
            "36c3f1c84733486bbf5e00b031c53c42",
            "377a2bd0227e4643918026c3c50813f0",
            "4728d43f6d6f43478b28e1116737c459",
            "4736bee7562a4db38705155b88452bfb",
            "3e75cc14123349d990d83e4d8dc671d8",
            "603fa47fc0cb42c78b5c161f20356ada",
            "641a3f0077ea4f96af51e7e4358880c5",
            "22c6aa2b6876412dab2e27741404f128",
            "9eb95f91251843618ecbbbf9524eb565",
            "92da051a4f394c05a113465f43dcadb1",
            "50d2dcf4ec6346a38c1ed2a32cd813a1",
            "f97942fdb06a47daaeddb2be3caa738f",
            "2c167d90bbb14066957cff1e42110ab8",
            "267708f24d1c4350b3d8293a473e286a",
            "5721f0a6d4764ccf85d31b06e8d8d4a4",
            "3676543a53c449c7b27d88e9bafcb5c3",
            "f20fee08e51a4d48a00863090076cf82",
            "b3cf2b78e2f448409d920487e78268dd",
            "74b0bebe95964bef830da4e2615d6542",
            "aa100c1e19dc483bbd7b67a7e6b30641",
            "b5fb4654c8974113ad39a3dc6ae074dc",
            "29b9d054ff394558a53103a3234fb2c6",
            "44f7139869e54528ab39eb2eef789f34"
          ]
        },
        "id": "TGZjJIA3iq5W",
        "outputId": "7e424403-f9c6-4990-a3c4-b328c5301337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2660198e4ba44d97ad27150b3f8f2f33"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "377a2bd0227e4643918026c3c50813f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c167d90bbb14066957cff1e42110ab8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bleu': 0.4971748899058618,\n",
              " 'precisions': [0.7009646302250804,\n",
              "  0.5399449035812672,\n",
              "  0.4410430839002268,\n",
              "  0.3660245183887916],\n",
              " 'brevity_penalty': 1.0,\n",
              " 'length_ratio': 1.0412946428571428,\n",
              " 'translation_length': 1866,\n",
              " 'reference_length': 1792}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "results = rouge.compute(predictions=predictions, references=references)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "8653459aedfc4eaa87b0219a9093dfc3",
            "90f59d47e52a4aae89741fb8e0ed255b",
            "faf9c1d8073b472b9a5be63929a72f8d",
            "9214f8b4129943d1a1fe0ffa4f679e61",
            "21c3f626c02343cb88b92181308e6320",
            "660a8507a0394b2a859affcd1d50f8b3",
            "942896fa0f9f45c099709ccd399cace0",
            "574ab20216be48589ecf16ff850fecd0",
            "e07b09824d954d53a08ed4f35e2c984b",
            "843e8b71bfed433bb5f03d1f57ebc49e",
            "614e6a22227d4af795b6468a422e356a"
          ]
        },
        "id": "Hs7OZnB8iuIB",
        "outputId": "6282f0b2-c69a-4d47-cae4-5c18664cadd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8653459aedfc4eaa87b0219a9093dfc3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': 0.7098545988484561,\n",
              " 'rouge2': 0.5437353173065924,\n",
              " 'rougeL': 0.6776760777142627,\n",
              " 'rougeLsum': 0.6778906338173573}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bertscore = evaluate.load('bertscore')\n",
        "results = bertscore.compute(predictions=predictions, references=references, lang='en')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296,
          "referenced_widgets": [
            "91abbfbae82540d1a3bc5a2974ada56c",
            "0f2461c3db904569ad48c54c1c29ef08",
            "fd66323f621e42159b5926446acbabed",
            "dbab766f7f994b97b6c4178482193839",
            "f0d105d9f4bf40efada05d739a7c4783",
            "f4ba7387fcf4469184e22fdb4c5aac12",
            "7210b194e5b0416ca3cf3a8df9fbd89e",
            "1c82a1d1ee67450582bc31508ea13639",
            "eee2370b9911447faf1bbdbbab06eb94",
            "ec52758e46f74ff3a145eaa4e5218592",
            "091bf6fad772414295a47cd6f70fdd1c",
            "6ac80b3da144464b96b08c6a24c40260",
            "3bbc12319bf54ee58196518a12202dc0",
            "c6bff44a953743038c2378afd2758113",
            "ec3c09416a1f47c88c43ba03c55b68ae",
            "3431e07b96dc4ab7a63095b35e60ec12",
            "37a3cab0fc844e4dbe28a596f4d85f3a",
            "26c31bee01104671880ae7079461126f",
            "b0dc3105d0ba49d487fb0b8a30e6e4cd",
            "4921198e9aca486c917d4631d59e80be",
            "70f729a760094e0fb02ae24923d94b0c",
            "7850d1f517194a379df4038b823e2fdf",
            "e332ff341e554c54958cdba710216339",
            "1157b41b1a704efc9d36ed9db143bb53",
            "c7b9f60644254a01afd2ee7dc6efb0d5",
            "992e01a743474418a7c9c3efb426955a",
            "7825e4a88bb34627959f3c20b357faed",
            "bc42da71e8774ba7b1f24afe8ea68cce",
            "e98448a1f10c4dcdaa6f9b2566d03849",
            "1bf5b36ffda548999cde20abb4c6c24f",
            "495268255ca3448da49006b1b4427cf3",
            "bf13f103c7384d7489553188cfd277ea",
            "3e0428e133e94f16bda3dfffa4d22964",
            "0e5c8f50806744f0907cf795ae256e9d",
            "ef5812ba81dc47c3b975deeb165032ee",
            "9188ead1adfe4cb694f4b2196cea5d64",
            "149f5141173a457884457f6878e14bd5",
            "4b407b056fab4fe4b159ff003210bc02",
            "df15c20be7ec4fd8afd7749c95ca9a03",
            "327294ec171a48b8ae4f56f480a88abe",
            "87d54db4376c42798da2b92623b863c9",
            "6ffff10b64b344b69c7834f5a7520e89",
            "5f49a90f565c4b3298bda42224fe89f9",
            "f04fd24ec7bc441692dc91ec51654638",
            "2bf30f61d2ee4213a6299bd92099df12",
            "fda6b5db6357437b92468e9d734d79ba",
            "5c8fc173d0d04970bf485b51a024ada8",
            "0e6e53a64b484c00a55748f1c6770ccc",
            "0cbd39ab68eb4cf4a9b1d68bb56a5867",
            "05002e21c9044a6597593dde8c09657c",
            "44d8dc8a92684cb5976ddc3ee5bcf3c8",
            "f212ffc5ef194eaebb3b1985eeb8c07f",
            "1d8e4b11d4124ac484004b07ee85970b",
            "abda29c226794900828d13d9ee766c15",
            "9996727760754ff08be94b809bd97a1e",
            "1c4e88e811944b51a0cb05c977f411a5",
            "97e8faaaea874776a831cb08b04c3489",
            "ff3a9102efe2417797244f51f117f1a8",
            "c9a87aab8d66435babf4b147a669476c",
            "873f921820c944028ca0a0e1ca9fd07b",
            "bf23f6c3a41c41a2b3ff503e485a91a2",
            "b4673d104bfc44059af8181e7e202a81",
            "95b44c6d64504dc39947fabc76175508",
            "baba8e7f8108418ca41bef04bab07cde",
            "2f6c404d42d04878988280c41a183271",
            "3049b998d3284f44ad33fe6af38dda58",
            "8a19cbe7179b4e99a9e75ae9163a9bb9",
            "175d9f351c2b450db6b1d227c04432b3",
            "86044823874a4b3588ddc2b88695acdb",
            "2fa7a926088b47b3b23ebe1c58cf283f",
            "384f67c4bb924d34915539e634afe0e5",
            "5ab6baa24873430194443d6bcd2d518d",
            "cf5c407914784c11b4a1a3d1fb069e59",
            "b117a230001c49dbb58a84d0e985833e",
            "b173b9d9e1da48338e532fcb7bb1ebfe",
            "c392244364874adb923de3f0b3a9bf0a",
            "dd11461b906e45fda8e6193e9256fd75"
          ]
        },
        "id": "q7PUn2jFiyC_",
        "outputId": "3da38fb4-af52-449a-a389-a12a8cf7ae1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91abbfbae82540d1a3bc5a2974ada56c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ac80b3da144464b96b08c6a24c40260"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e332ff341e554c54958cdba710216339"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e5c8f50806744f0907cf795ae256e9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2bf30f61d2ee4213a6299bd92099df12"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c4e88e811944b51a0cb05c977f411a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a19cbe7179b4e99a9e75ae9163a9bb9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(results['precision'])/len(results['precision'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_7MkiLVi25o",
        "outputId": "d06a31f5-1f3a-4fdf-abdd-b267fe8824cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9607064373352948"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(results['recall'])/len(results['recall'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBfxp6DTi8wn",
        "outputId": "b2016c75-185c-495b-9690-b7348eeeccd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9615864613476921"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(results['f1'])/len(results['f1'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adjlwke-i9VN",
        "outputId": "c2130c0f-6708-4039-c458-4cbeab3d905d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9610980269955653"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phi-4 Few (4) - Shot"
      ],
      "metadata": {
        "id": "-0E8CBT-jAal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot = pd.read_csv('/content/drive/Shareddrives/FYP 2024-2025/Phase-2/model-evaluation/phi_4_few_shot_processed.csv')\n",
        "# few_shot.columns\n",
        "references = few_shot[\"debiased_sentence\"].tolist()\n",
        "predictions = few_shot[\"model_output\"].tolist()"
      ],
      "metadata": {
        "id": "LHU-XueVjEuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = meteor.compute(predictions=predictions, references=references)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yXZkAgAjL2b",
        "outputId": "ba5c7710-373a-41ca-fab7-63015faaa7fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'meteor': 0.7888987446071363}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = bleu.compute(predictions=predictions, references=references)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DUfOr4WjT8g",
        "outputId": "3dc59824-a647-4b91-d573-0e2891bea4de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bleu': 0.6133556543428611,\n",
              " 'precisions': [0.7659688674181427,\n",
              "  0.6456953642384106,\n",
              "  0.5689948892674617,\n",
              "  0.5029239766081871],\n",
              " 'brevity_penalty': 1.0,\n",
              " 'length_ratio': 1.0396205357142858,\n",
              " 'translation_length': 1863,\n",
              " 'reference_length': 1792}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = rouge.compute(predictions=predictions, references=references)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSHQ8kIBjWkW",
        "outputId": "ef2d01e8-ef9e-42a3-ea4f-2cf4d8202967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': 0.7721853401087778,\n",
              " 'rouge2': 0.6510008314033278,\n",
              " 'rougeL': 0.756316382126532,\n",
              " 'rougeLsum': 0.7555003453763642}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = bertscore.compute(predictions=predictions, references=references, lang='en')"
      ],
      "metadata": {
        "id": "LFdWqtVvjZii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(results['precision'])/len(results['precision'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPM35NE_jcgX",
        "outputId": "096a937a-ea49-4a42-d5e2-31c45c0494c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9686842163403829"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(results['recall'])/len(results['recall'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZUlYtxujhQx",
        "outputId": "dd92a3bb-51f3-495d-9119-77dd7780d009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9681981042319653"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(results['f1'])/len(results['f1'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MweCtoejjkks",
        "outputId": "1fc69708-801b-4ad3-e4ec-8a181240ce0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9683819857298159"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}